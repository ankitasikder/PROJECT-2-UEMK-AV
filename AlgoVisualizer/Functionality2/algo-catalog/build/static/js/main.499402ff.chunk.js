(this.webpackJsonptutorial=this.webpackJsonptutorial||[]).push([[0],{26:function(e,t,s){},33:function(e,t,s){"use strict";s.r(t);var i=s(1),c=s.n(i),r=s(16),a=s.n(r),n=(s(26),s(14)),l=s.p+"static/media/Bricksort.c878483c.jpg",o=s.p+"static/media/Binaryinsertion.106fbe11.jpg",h=s.p+"static/media/Pegionhole.99d3e7c2.jpg",d=s.p+"static/media/Bubble.ef2facea.jpg",j=s.p+"static/media/Selection.baa7cfa8.jpg",b=s.p+"static/media/Quick.0502ef54.jpg",O=s.p+"static/media/Insertion.5609f2ae.jpg",p=s.p+"static/media/Merge.6afb5fd7.jpg",m=s.p+"static/media/Heap.b7621843.jpg",x=s.p+"static/media/Counting.34268ae0.jpg",u=s.p+"static/media/Radix.52899699.jpg",g=s.p+"static/media/Bucket.d474e6a0.jpg",f=s.p+"static/media/Comb.677065fe.jpg",v=s.p+"static/media/Tim.9ca30e64.jpg",y=s.p+"static/media/Cycle.4c5828a8.jpg",w=s.p+"static/media/Bitonic.e9cc5386.jpg",T=s.p+"static/media/Sleep.6aea81eb.jpg",S=s.p+"static/media/Coctail.3fa7de26.jpg",E=s.p+"static/media/Strand.a9f20852.jpg",I=s.p+"static/media/Shell.e992e235.jpg",A=s.p+"static/media/TREE.2d668805.jpg",C=s.p+"static/media/Permutation.16db609d.jpg",k=s.p+"static/media/Pancake.13580829.jpg",R=s.p+"static/media/Gnome.4511c0c8.jpg",M=s(0),B=d,L=j,N=b,P=O,V=p,G=m,H=x,F=u,q=g,D=f,Y=v,X=y,z=w,W=T,K=S,U=E,J=I,Q=A,Z=C,_=k,$=R,ee=l,te=o,se=h,ie={img:B,title:"BUBBLE SORT",alt:"firstSortingBubbleSort"},ce={img:L,title:"SELECTION SORT",alt:"secondSortingSelectionSort"},re={img:N,title:"QUICK SORT",alt:"thirdSortingQuickSort"},ae={img:P,title:"INSERTION SORT",alt:"fourthSortingInsertionSort"},ne={img:V,title:"MERGE SORT",alt:"fifthSortingMergeSort"},le={img:G,title:"HEAP SORT",alt:"sixthSortingHeapsort"},oe={img:H,title:"COUNTING SORT",alt:"seventhSortingCountingSort"},he={img:F,title:"RADIX SORT",alt:"eighthSortingRadixSort"},de={img:q,title:"BUCKET SORT",alt:"ninethSortingBucketSort"},je={img:D,title:"COMB SORT",alt:"tenthSortingCombSort"},be={img:Y,title:"TIM SORT",alt:"eleventhSortingTimSort"},Oe={img:X,title:"CYCLE SORT",alt:"twelvethSortingCycleSort"},pe={img:z,title:"BITONIC SORT",alt:"thirteenthSortingBitonicSort"},me={img:W,title:"SLEEP SORT",alt:"fourteenthSortingSleepSort"},xe={img:K,title:"COCTAIL SORT",alt:"fifteenthSortingCoctailSort"},ue={img:U,title:"STRAND SORT",alt:"sixteenthSortingStrandSort"},ge={img:J,title:"SHELL SORT",alt:"seventeenthSortingShellSort"},fe={img:Q,title:"TREE SORT",alt:"eighteenthSortingTreeSort"},ve={img:Z,title:"PERMUTATION SORT",alt:"nineteenthSortingPermutationSort"},ye={img:_,title:"PANCAKE SORT",alt:"twentythSortingPancakeSort"},we={img:$,title:"Gnome Sort",alt:"twentyonethSortingGnomeSort"},Te={img:ee,title:"EVEN-ODD SORT/BRICK SORT ALGORITHM",alt:"twentysecondSortingbricksort"},Se={img:te,title:"BINARY INSERTION SORT ALGORITHM",alt:"twentythirdSortingbinaryinsertionsort"},Ee={img:se,title:"PEGIONHOLE SORT ALGORITHM",alt:"twentyfourthSortingpegionhole"},Ie=[{id:1,img:ie.img,title:ie.title,alt:ie.alt,specificcations:Object(M.jsx)("p",{children:"Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order"})},{id:2,img:ce.img,title:ce.title,alt:ce.alt,specificcations:Object(M.jsx)("p",{children:"The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning."})},{id:3,img:re.img,title:re.title,alt:re.alt,specificcations:Object(M.jsx)("p",{children:"Quick sort is a highly efficient sorting algorithm and is based on partitioning of array of data into smaller arrays."})},{id:4,img:ae.img,title:ae.title,alt:ae.alt,specificcations:Object(M.jsx)("p",{children:"This is an in-place comparison-based sorting algorithm. Here, a sub-list is maintained which is always sorted."})},{id:5,img:ne.img,title:ne.title,alt:ne.alt,specificcations:Object(M.jsx)("p",{children:"Merge Sort is a Divide and Conquer algorithm"})},{id:6,img:le.img,title:le.title,alt:le.alt,specificcations:Object(M.jsx)("p",{children:"Heap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap."})},{id:7,img:oe.img,title:oe.title,alt:oe.alt,specificcations:Object(M.jsx)("p",{children:"Counting sort is a sorting technique based on keys between a specific range. It works by counting the number of objects having distinct key values (kind of hashing). Then doing some arithmetic to calculate the position of each object in the output sequence."})},{id:8,img:he.img,title:he.title,alt:he.alt,specificcations:Object(M.jsx)("p",{children:"The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort."})},{id:9,img:de.img,title:de.title,alt:de.alt,specificcations:Object(M.jsx)("p",{children:"Bucket Sort is a sorting technique that sorts the elements by first dividing the elements into several groups called buckets. The elements inside each bucket are sorted using any of the suitable sorting algorithms or recursively calling the same algorithm."})},{id:10,img:je.img,title:je.title,alt:je.alt,specificcations:Object(M.jsx)("p",{children:"The basic idea of comb sort and the bubble sort is same. In other words, comb sort is an improvement on the bubble sort"})},{id:11,img:be.img,title:be.title,alt:be.alt,specificcations:Object(M.jsx)("p",{children:"Tim-sort is a sorting algorithm derived from insertion sort and merge sort. It was designed to perform in an optimal way on different kind of real world data."})},{id:12,img:Oe.img,title:Oe.title,alt:Oe.alt,specificcations:Object(M.jsx)("p",{children:"Cycle sort is a comparison sorting algorithm which forces array to be factored into the number of cycles where each of them can be rotated to produce a sorted array. It is theoretically optimal in the sense that it reduces the number of writes to the original array."})},{id:13,img:pe.img,title:pe.title,alt:pe.alt,specificcations:Object(M.jsx)("p",{children:"Bitonic Sort is a classic parallel algorithm for sorting."})},{id:14,img:me.img,title:me.title,alt:me.alt,specificcations:Object(M.jsx)("p",{children:"sleep sort works by starting a separate task for each item to be sorted, where each task sleeps for an interval corresponding to the item's sort key, then emits the item. Items are then collected sequentially in time."})},{id:15,img:xe.img,title:xe.title,alt:xe.alt,specificcations:Object(M.jsx)("p",{children:"Cocktail Sort is a variation of Bubble sort. The Bubble sort algorithm always traverses elements from left and moves the largest element to its correct position in first iteration and second largest in second iteration and so on. "})},{id:16,img:ue.img,title:ue.title,alt:ue.alt,specificcations:Object(M.jsx)("p",{children:"Strand sort is a recursive sorting algorithm that sorts items of a list into increasing order. It has O(n2) worst time complexity which occurs when the input list is reverse sorted."})},{id:17,img:ge.img,title:ge.title,alt:ge.alt,specificcations:Object(M.jsx)("p",{children:"Shell sort is an algorithm that first sorts the elements far apart from each other and successively reduces the interval between the elements to be sorted. It is a generalized version of insertion sort."})},{id:18,img:fe.img,title:fe.title,alt:fe.alt,specificcations:Object(M.jsx)("p",{children:"Tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order"})},{id:19,img:ve.img,title:ve.title,alt:ve.alt,specificcations:Object(M.jsx)("p",{children:"BogoSort also known as permutation sort, stupid sort, slow sort, shotgun sort or monkey sort is a particularly ineffective algorithm based on generate and test paradigm. The algorithm successively generates permutations of its input until it finds one that is sorted."})},{id:20,img:ye.img,title:ye.title,alt:ye.alt,specificcations:Object(M.jsx)("p",{children:"Pancake sort is called so because it resembles sorting pancakes on a plate with a spatula, where you can only use the spatula to flip some of the top pancakes in the plate."})},{id:21,img:we.img,title:we.title,alt:we.alt,specificcations:Object(M.jsx)("p",{children:"Gnome Sort also called Stupid sort is based on the concept of a Garden Gnome sorting his flower pots."})},{id:22,img:Te.img,title:Te.title,alt:Te.alt,specificcations:Object(M.jsx)("p",{children:"In computing, an odd\u2013even sort or odd\u2013even transposition sort (also known as brick sort or parity sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections."})},{id:23,img:Se.img,title:Se.title,alt:Se.alt,specificcations:Object(M.jsx)("p",{children:"Binary search is used to reduce the number of comparisons in Insertion sort."})},{id:24,img:Ee.img,title:Ee.title,alt:Ee.alt,specificcations:Object(M.jsx)("p",{children:"It is similar to counting sort, but differs in that it \u201cmoves items twice: once to the bucket array and again to the final destination \u201c."})}],Ae=s.p+"static/media/sublistsearch.7f94fa65.jpg",Ce=s.p+"static/media/Fibonaccisearch.bd594a32.jpg",ke=s.p+"static/media/Ksearch.1f77e3b9.jpg",Re=s.p+"static/media/Ubiquitous.0ef5ff40.jpg",Me=s.p+"static/media/Exponentialsearch.27958edd.jpg",Be=s.p+"static/media/Linearsearch.34dea789.jpg",Le=s.p+"static/media/Binarysearch.23cb6d94.jpg",Ne=s.p+"static/media/Jumpsearch.7cdc0e0a.jpg",Pe=s.p+"static/media/Interpolationsearch.87051c09.jpg",Ve=s.p+"static/media/Asearch.28883189.jpg",Ge=s.p+"static/media/Breadthsearch.b2fc5f79.jpg",He=s.p+"static/media/Depthsearch.5708abc6.jpg",Fe=Be,qe=Ne,De=Pe,Ye=Le,Xe=Me,ze=Ae,We=Ce,Ke=ke,Ue=Re,Je=Ve,Qe={img:Fe,title:"LINEAR SEARCH",alt:"firstSearchingAlgo"},Ze={img:qe,title:"JUMP SEARCH",alt:"secondSearchingAlgo"},_e={img:De,title:"INTERPOLATION SEARCH",alt:"thirdSearchingAlgo"},$e={img:Ye,title:"BINARY SEARCH",alt:"fourthSearchingBinarySearch"},et={img:Xe,title:"EXPONENTIAL SEARCH",alt:"fifthSearchingAlgo"},tt={img:ze,title:"SUBLIST SEARCH(SEARCH A LINKED LIST IN ANOTHER LIST)",alt:"sixthSearchingAlgo"},st={img:We,title:"FIBONACCI SEARCH",alt:"seventhSearchingAlgo"},it={img:Ke,title:"K SEARCH",alt:"eighthSearchingAlgo"},ct={img:Ue,title:"UBIQUITOUS BINARY SEARCH",alt:"ninthSearchingAlgo"},rt={img:Je,title:"A*SEARCH",alt:"tenthSearchingastarsearch"},at={img:Ge,title:"BREADTHE FIRST SEARCH",alt:"eleventhSearchingbfs"},nt={img:He,title:"DEPTH FIRST SEARCH",alt:"twelvthSearchingdfs"},lt=[{id:1,img:Qe.img,title:Qe.title,alt:Qe.alt,specificcations:Object(M.jsx)("p",{children:"In computer science, a linear search or sequential search is a method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched."})},{id:2,img:Ze.img,title:Ze.title,alt:Ze.alt,specificcations:Object(M.jsx)("p",{children:"In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items L\u2096\u2098, where k\\in \\mathbb  and m is the block size, until an item is found that is larger than the search key."})},{id:3,img:_e.img,title:_e.title,alt:_e.alt,specificcations:Object(M.jsx)("p",{children:"Interpolation search is an algorithm for searching for a key in an array that has been ordered by numerical values assigned to the keys. It was first described by W. W. Peterson in 1957."})},{id:4,img:$e.img,title:$e.title,alt:$e.alt,specificcations:Object(M.jsx)("p",{children:"In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. "})},{id:5,img:et.img,title:et.title,alt:et.alt,specificcations:Object(M.jsx)("p",{children:"In computer science, an exponential search is an algorithm, created by Jon Bentley and Andrew Chi-Chih Yao in 1976, for searching sorted, unbounded/infinite lists."})},{id:6,img:tt.img,title:tt.title,alt:tt.alt,specificcations:Object(M.jsx)("p",{children:"The sublist search algorithm works by comparing the first element of the first list with the first element of the second list. If the two values don't match, it goes to the next element of the second list. "})},{id:7,img:st.img,title:st.title,alt:st.alt,specificcations:Object(M.jsx)("p",{children:"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers."})},{id:8,img:it.img,title:it.title,alt:it.alt,specificcations:Object(M.jsx)("p",{children:"A step array is an array of integer where each element has a difference of at most k with its neighbor. Given a key x, we need to find the index value of x if multiple element exist return the first occurrence of the key."})},{id:9,img:ct.img,title:ct.title,alt:ct.alt,specificcations:Object(M.jsx)("p",{children:"We all aware of binary search algorithm. Binary search is easiest difficult algorithm to get it right. I present some interesting problems that I collected on binary search. There were some requests on binary search."})},{id:10,img:rt.img,title:rt.title,alt:rt.alt,specificcations:Object(M.jsx)("p",{children:"A* Search algorithm is one of the best and popular technique used in path-finding and graph traversals."})},{id:11,img:at.img,title:at.title,alt:at.alt,specificcations:Object(M.jsx)("p",{children:"BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse."})},{id:12,img:nt.img,title:nt.title,alt:nt.alt,specificcations:Object(M.jsx)("p",{children:"A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tr\xe9maux."})}],ot=s(8),ht=s.p+"static/media/DIJKSTRAALGORITHM.90c40874.jpg",dt=s.p+"static/media/Astaralgo.7335c6f3.jpg",jt=s.p+"static/media/Amicable.3414b2a3.jpg",bt=s.p+"static/media/Bellman.62b45368.jpg",Ot=s.p+"static/media/Biconnected.85b0b881.jpg",pt=s.p+"static/media/Bitwise.e0a4f6bb.jpg",mt=s.p+"static/media/Boruvka.a17494a7.jpg",xt=s.p+"static/media/Breadthfirstalgo.fc2e2f58.jpg",ut=s.p+"static/media/Brent.9eef67f0.jpg",gt=s.p+"static/media/Chinese.37112e29.jpg",ft=s.p+"static/media/Cooley.d78a2a00.jpg",vt=s.p+"static/media/Depthfirstalgo.78174159.jpg",yt=s.p+"static/media/Divide.745d9d1a.jpg",wt=s.p+"static/media/Edmond.81356005.jpg",Tt=s.p+"static/media/Eularian.38d4b407.jpg",St=s.p+"static/media/Fleurys.84dd56a1.jpg",Et=s.p+"static/media/Floodfill.ff7d08bd.jpg",It=s.p+"static/media/Floyd.5653fe42.jpg",At=s.p+"static/media/Ford.dfafa616.jpg",Ct=s.p+"static/media/Graph.abd6c183.jpg",kt=s.p+"static/media/Greedy.737aac61.jpg",Rt=s.p+"static/media/Hamiltonian.067987a1.jpg",Mt=s.p+"static/media/Hoffcroft.42b142fd.jpg",Bt=s.p+"static/media/Huffman.ef54df82.jpg",Lt=s.p+"static/media/Jhonson.3bd8f2ed.jpg",Nt=s.p+"static/media/Karatsuba.8a51abcd.jpg",Pt=s.p+"static/media/Kargers.370edb3c.jpg",Vt=s.p+"static/media/Kruskal.63a700bb.jpg",Gt=s.p+"static/media/Ksmall.446bf57c.jpg",Ht=s.p+"static/media/Prims.b0be5a7a.jpg",Ft=s.p+"static/media/Randomized.f18a8530.jpg",qt=s.p+"static/media/Reverse.808583f3.jpg",Dt=s.p+"static/media/Strassen.a0c8db8d.jpg",Yt=s.p+"static/media/TARJAN.8a968dd1.jpg",Xt=s.p+"static/media/Toopologicalalgo.e3cfddb8.jpg",zt=s.p+"static/media/Transitive.3b195a5d.jpg",Wt={img:ht,title:"DIJKSTRA'S ALGORITHM",alt:"firstOtherDIJKSTRA"},Kt={img:It,title:"FLOYD WARSHALL'S ALGORITHM",alt:"secondOtherFLOYDWARSHALL"},Ut={img:dt,title:"A*SEARCH ALGORITHM",alt:"thirdOtherASTARSEARCH"},Jt={img:xt,title:"BREADTHE FIRST SEARCH ALGORITHM",alt:"fourthOtherbfs"},Qt={img:vt,title:"DEPTH FIRST SEARCH ALGORITHM",alt:"fifthOtherdfs"},Zt={img:kt,title:"GREEDY ALGORITHM",alt:"sixthOthergreedy"},_t={img:Bt,title:"HUFFMAN ALGORITHM",alt:"seventhOtherhofman"},$t={img:pt,title:"BITWISE ALGORITHM",alt:"eighthOtherbit"},es={img:Ct,title:"GRAPH ALGORITHM",alt:"ninthOthergraph"},ts={img:Ft,title:"RANDOMIZED ALGORITHM",alt:"tenthOtherrandomized"},ss={img:Pt,title:"KARGERS ALGORITHM",alt:"eleventhOtherkargers"},is={img:Rt,title:"HAMILTONIAN CYCLE ALGORITHM",alt:"twelvthOtherhamiltoniancycle"},cs={img:St,title:"FLEURYS ALGORITHM",alt:"thirteenthOtherfleurys"},rs={img:Tt,title:"EULARIAN PATH ALGORITHM",alt:"fourteenthOthereularianpath"},as={img:Yt,title:"TARJAN'S ALGORITHM",alt:"fifteenthOthertarjan"},ns={img:zt,title:"TRANSITIVE CLOSURE",alt:"sixteenOthertransitiveclosure"},ls={img:Ot,title:"BICONNECTED GRAPH",alt:"seventeenOtherbiconnectedgraph"},os={img:Lt,title:"JOHNSON ALGORITHM",alt:"eightteenOtherjohnsons"},hs={img:bt,title:"BELLMAN FORD ALGORITHM",alt:"nineteenOtherbellmanford"},ds={img:At,title:"FORD OF FALLCURSON ALGORITHM",alt:"twentyOtherfordfallcurson"},js={img:Mt,title:"HOFFCROFT KARP ALGORITHM",alt:"twentyoneOtherhoffcroftkarp"},bs={img:mt,title:"BORUVKA",alt:"twentytwoOtherboruvka"},Os={img:Nt,title:"KARATSUBA",alt:"twentythreeOtherkaratsuba"},ps={img:Et,title:"FLOODFILL",alt:"twentyfourOtherfloodfill"},ms={img:Vt,title:"KRUSKAL",alt:"twentyfiveOtherkruskal"},xs={img:Xt,title:"TOPOLOGICAL SORTING",alt:"twentysixOthertopologicalsorting"},us={img:Ht,title:"PRIMS MINIMUM SPANNING TREE",alt:"twentysevenOtherprimsminimumspanningtree"},gs={img:Gt,title:"K SMALLEST",alt:"twentyeightOtherksmallest"},fs={img:Dt,title:"STRASSEN's",alt:"twentynineOtherstrassen"},vs={img:ft,title:"Cooley Tukey Fast Fourier Transform",alt:"thirtyOthercooleytukeyfastfouriertransform"},ys={img:yt,title:"DIVIDE & CONQUROR",alt:"thirtyoneOtherdivideconquror"},ws={img:gt,title:"CHINESE REMAINDER THEOREM",alt:"thirtytwoOtherchineseremaindertheorem"},Ts={img:jt,title:"AMICABLE PAIRS",alt:"thirtythreeOtheramicablepairs"},Ss={img:ut,title:"BRENT'S ALGORITHM",alt:"thirtifourthOtherBRENT"},Es={img:qt,title:"REVERSE DELETE ALGORITHM",alt:"thirtififthOtherREVERSEDELETE"},Is={img:wt,title:"EDMOND'S ALGORITHM",alt:"thirtisixthOtherEDMOND"};function As(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"progress-container",children:Object(M.jsx)("div",{class:"progress-bar",id:"myBar"})}),Object(M.jsx)("center",{children:Object(M.jsx)("div",{className:"godText",children:Object(M.jsx)("h1",{children:"SEARCHING ALGORITHMS"})})}),Object(M.jsx)("center",{children:Object(M.jsx)("section",{className:"algolist",children:lt.map((function(e,t){e.img,e.title,e.specificcations;return Object(M.jsx)(Rs,Object(n.a)({},e),e.id)}))})}),Object(M.jsx)("br",{}),Object(M.jsx)("br",{}),Object(M.jsx)("center",{children:Object(M.jsx)("div",{className:"godText",children:Object(M.jsx)("h1",{children:"SORTING ALGORITHMS"})})}),Object(M.jsx)("center",{children:Object(M.jsx)("section",{className:"algolist",children:Ie.map((function(e,t){e.img,e.title,e.specificcations;return Object(M.jsx)(ks,Object(n.a)({},e),e.id)}))})}),Object(M.jsx)("br",{}),Object(M.jsx)("br",{}),Object(M.jsx)("center",{children:Object(M.jsx)("div",{className:"godText",children:Object(M.jsx)("h1",{children:"OTHERS ALGORITHMS"})})}),Object(M.jsx)("certer",{children:Object(M.jsxs)("section",{className:"algolist",children:[Object(M.jsx)(Cs,{img:Wt.img,title:Wt.title,alt:Wt.alt,children:Object(M.jsx)("p",{children:"Dijkstra\u2019s algorithm is very similar to Prim\u2019s algorithm for minimum spanning tree. "})}),Object(M.jsx)(Cs,{img:Kt.img,title:Kt.title,alt:Kt.alt,children:Object(M.jsx)("p",{children:"The Floyd Warshall Algorithm is for solving the All Pairs Shortest Path problem. "})}),Object(M.jsx)(Cs,{img:Ut.img,title:Ut.title,alt:Ut.alt,children:Object(M.jsx)("p",{children:"A* Search algorithm is one of the best and popular technique used in path-finding and graph traversals."})}),Object(M.jsx)(Cs,{img:Jt.img,title:Jt.title,alt:Jt.alt,children:Object(M.jsx)("p",{children:"BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse."})}),Object(M.jsx)(Cs,{img:Qt.img,title:Qt.title,alt:Qt.alt,children:Object(M.jsx)("p",{children:"A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tr\xe9maux."})}),Object(M.jsx)(Cs,{img:Zt.img,title:Zt.title,alt:Zt.alt,children:Object(M.jsx)("p",{children:"Greedy is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. "})}),Object(M.jsx)(Cs,{img:_t.img,title:_t.title,alt:_t.alt,children:Object(M.jsx)("p",{children:'The process of finding or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes"'})}),Object(M.jsx)(Cs,{img:$t.img,title:$t.title,alt:$t.alt,children:Object(M.jsx)("p",{children:"The bitwise operations are found to be much faster and are some times used to improve the efficiency of a program."})}),Object(M.jsx)(Cs,{img:es.img,title:es.title,alt:es.alt,children:Object(M.jsx)("p",{children:"A graph is an abstract notation used to represent the connection between pairs of objects. "})}),Object(M.jsx)(Cs,{img:ts.img,title:ts.title,alt:ts.alt,children:Object(M.jsx)("p",{children:"An algorithm that uses random numbers to decide what to do next anywhere in its logic is called Randomized Algorithm. "})}),Object(M.jsx)(Cs,{img:ss.img,title:ss.title,alt:ss.alt,children:Object(M.jsx)("p",{children:"Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph."})}),Object(M.jsx)(Cs,{img:is.img,title:is.title,alt:is.alt,children:Object(M.jsx)("p",{children:"In the mathematical field of graph theory, a Hamiltonian path (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once."})}),Object(M.jsx)(Cs,{img:cs.img,title:cs.title,alt:cs.alt,children:Object(M.jsx)("p",{children:"Fleury\u2019s Algorithm is used to display the Euler path or Euler circuit from a given graph."})}),Object(M.jsx)(Cs,{img:rs.img,title:rs.title,alt:rs.alt,children:Object(M.jsx)("p",{children:"A trail in a finite graph that visits every edge exactly once (allowing for revisiting vertices)"})}),Object(M.jsx)(Cs,{img:as.img,title:as.title,alt:as.alt,children:Object(M.jsx)("p",{children:"An algorithm in graph theory for finding the strongly connected components (SCCs) of a directed graph"})}),Object(M.jsx)(Cs,{img:ns.img,title:ns.title,alt:ns.alt,children:Object(M.jsx)("p",{children:"Transitive Closure it the reachability matrix to reach from vertex u to vertex v of a graph. One graph is given, we have to find a vertex v which is reachable from another vertex u, for all vertex pairs (u, v)."})}),Object(M.jsx)(Cs,{img:ls.img,title:ls.title,alt:ls.alt,children:Object(M.jsx)("p",{children:'In graph theory, a biconnected graph is a connected and "nonseparable" graph, meaning that if any one vertex were to be removed, the graph will remain connected.'})}),Object(M.jsx)(Cs,{img:os.img,title:os.title,alt:os.alt,children:Object(M.jsx)("p",{children:"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist."})}),Object(M.jsx)(Cs,{img:hs.img,title:hs.title,alt:hs.alt,children:Object(M.jsx)("p",{children:"The Bellman\u2013Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph."})}),Object(M.jsx)(Cs,{img:ds.img,title:ds.title,alt:ds.alt,children:Object(M.jsx)("p",{children:"The Ford-Fulkerson algorithm is used to detect maximum flow from start vertex to sink vertex in a given graph."})}),Object(M.jsx)(Cs,{img:js.img,title:js.title,alt:js.alt,children:Object(M.jsx)("p",{children:" The Hopcroft\u2013Karp algorithm (sometimes more accurately called the Hopcroft\u2013Karp\u2013Karzanov algorithm) is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching \u2013 a set of as many edges as possible with the property that no two edges share an endpoint. "})}),Object(M.jsx)(Cs,{img:bs.img,title:bs.title,alt:bs.alt,children:Object(M.jsx)("p",{children:"Bor\u016fvka's algorithm is a greedy algorithm for finding a minimum spanning tree in a graph, or a minimum spanning forest in the case of a graph that is not connected."})}),Object(M.jsx)(Cs,{img:Os.img,title:Os.title,alt:Os.alt,children:Object(M.jsx)("p",{children:"The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatoly Karatsuba in 1960 and published in 1962."})}),Object(M.jsx)(Cs,{img:ps.img,title:ps.title,alt:ps.alt,children:Object(M.jsx)("p",{children:"Flood fill, also called seed fill, is an algorithm that determines and alters the area connected to a given node in a multi-dimensional array with some matching attribute."})}),Object(M.jsx)(Cs,{img:ms.img,title:ms.title,alt:ms.alt,children:Object(M.jsx)("p",{children:"Kruskal's algorithm finds a minimum spanning forest of an undirected edge-weighted graph. If the graph is connected, it finds a minimum spanning tree."})}),Object(M.jsx)(Cs,{img:xs.img,title:xs.title,alt:xs.alt,children:Object(M.jsx)("p",{children:"Topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering."})}),Object(M.jsx)(Cs,{img:us.img,title:us.title,alt:us.alt,children:Object(M.jsx)("p",{children:"Prim's (also known as Jarn\xedk's) algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph."})}),Object(M.jsx)(Cs,{img:gs.img,title:gs.title,alt:gs.alt,children:Object(M.jsx)("p",{children:"A selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. "})}),Object(M.jsx)(Cs,{img:fs.img,title:fs.title,alt:fs.alt,children:Object(M.jsx)("p",{children:"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication."})}),Object(M.jsx)(Cs,{img:vs.img,title:vs.title,alt:vs.alt,children:Object(M.jsx)("p",{children:"Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm."})}),Object(M.jsx)(Cs,{img:ys.img,title:ys.title,alt:ys.alt,children:Object(M.jsx)("p",{children:"A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly."})}),Object(M.jsx)(Cs,{img:ws.img,title:ws.title,alt:ws.alt,children:Object(M.jsx)("p",{children:"The Chinese remainder theorem is a theorem which gives a unique solution to simultaneous linear congruences with coprime moduli."})}),Object(M.jsx)(Cs,{img:Ts.img,title:Ts.title,alt:Ts.alt,children:Object(M.jsx)("p",{children:"Amicable numbers are two different numbers so related that the sum of the proper divisors of each is equal to the other number."})}),Object(M.jsx)(Cs,{img:Ss.img,title:Ss.title,alt:Ss.alt,children:Object(M.jsx)("p",{children:"Brent\u2019s cycle detection algorithm is similar to floyd\u2019s algorithm as it also uses two pointer technique. But there is some difference in their approaches."})}),Object(M.jsx)(Cs,{img:Es.img,title:Es.title,alt:Es.alt,children:Object(M.jsx)("p",{children:"This algorithm is a greedy algorithm, choosing the best choice given any situation. It is the reverse of Kruskal's algorithm."})}),Object(M.jsx)(Cs,{img:Is.img,title:Is.title,alt:Is.alt,children:Object(M.jsx)("p",{children:"The algorithm was proposed independently first by Yoeng-Jin Chu and Tseng-Hong Liu (1965) and then by Jack Edmonds (1967)."})})]})})]})}var Cs=function(e){var t=e.img,s=e.title,i=e.alt;e.children;return Object(M.jsxs)("article",{className:"algo",children:[Object(M.jsx)("center",{children:Object(M.jsx)("img",{src:t,height:"230px",width:"300px",style:{borderRadius:20},title:s})}),Object(M.jsx)("center",{children:Object(M.jsx)("h1",{style:{color:"#810034"},onMouseOver:function(){console.log(s)},children:s})}),Object(M.jsx)("center",{children:e.children}),Object(M.jsx)("br",{}),Object(M.jsx)("center",{children:Object(M.jsx)(ot.b,{to:"/".concat(i),children:Object(M.jsx)("button",{class:"button",children:"Learn More"})})})]})},ks=function(e){var t=e.img,s=e.title,i=e.alt,c=e.specificcations;return Object(M.jsxs)("article",{className:"algo",children:[Object(M.jsx)("center",{children:Object(M.jsx)("img",{src:t,height:"230px",width:"300px",style:{borderRadius:20},title:s})}),Object(M.jsx)("center",{children:Object(M.jsx)("h1",{style:{color:"#810034"},children:s})}),Object(M.jsx)("center",{children:c}),Object(M.jsx)("center",{children:Object(M.jsx)(ot.b,{to:"/".concat(i),children:Object(M.jsx)("button",{class:"button",children:"Learn More"})})})]})},Rs=function(e){var t=e.img,s=e.title,i=e.alt,c=e.specificcations;return Object(M.jsxs)("article",{className:"algo",children:[Object(M.jsx)("center",{children:Object(M.jsx)("img",{src:t,height:"230px",width:"300px",style:{borderRadius:20},title:s})}),Object(M.jsx)("center",{children:Object(M.jsx)("h1",{style:{color:"#810034"},children:s})}),Object(M.jsx)("center",{children:c}),Object(M.jsx)("center",{children:Object(M.jsx)(ot.b,{to:"/".concat(i),children:Object(M.jsx)("button",{class:"button",children:"Learn More"})})})]})},Ms=s(2),Bs=s(18),Ls=s(19),Ns=s(21),Ps=s(20),Vs=function(e){Object(Ns.a)(s,e);var t=Object(Ps.a)(s);function s(e){var i;return Object(Bs.a)(this,s),(i=t.call(this,e)).state={hidden:!0},i}return Object(Ls.a)(s,[{key:"componentDidMount",value:function(){var e=this;setTimeout((function(){e.setState({hidden:!1})}),this.props.waitBeforeShow)}},{key:"render",value:function(){return this.state.hidden?"":this.props.children}}]),s}(c.a.Component);var Gs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"DIJKSTRA's ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for finding the shortest paths between nodes in a graph"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"WORST::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"AVERAGE::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::\u0398(V) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, "}),Object(M.jsx)("p",{children:"which may represent, for example, road networks."}),Object(M.jsx)("p",{children:"It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later."})]})]})]})})]})};var Hs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"FLOYD WARSHAL's ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for finding shortest paths in a directed weighted graph with positive or negative edge weights "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE:: O(V^3) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(|V|2) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In computer science, the Floyd\u2013Warshall algorithm (also known as Floyd's algorithm, the Roy\u2013Warshall algorithm, the Roy\u2013Floyd algorithm, or the WFI algorithm)is an algorithm for finding shortest paths in a directed weighted graph with positive or negative edge weights (but with no negative cycles)."}),Object(M.jsx)("p",{children:"A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. "}),Object(M.jsx)("p",{children:" Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm."})]})]})]})})]})};var Fs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"A*SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(bd) where b is the branching factor (the average number of successors per state)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(|V|) = O(b^d) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:'A* (pronounced "A-star") is a graph traversal and path search algorithm. A* is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency.'}),Object(M.jsx)("p",{children:" One major practical drawback is its O(b^d) space complexity, as it stores all generated nodes in memory."}),Object(M.jsx)("p",{children:"Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance."})]})]})]})})]})};var qs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BREADTH FIRST SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for traversing or searching tree or graph data structures"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::O(N) where N is the number of nodes"}),Object(M.jsx)("p",{children:"WORST::O(N) where N is the number of nodes"}),Object(M.jsx)("p",{children:"AVERAGE::O(N) where N is the number of nodes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"BEST::Worst Case for DFS will be the best case for BFS"}),Object(M.jsx)("p",{children:"WORST::Best Case for DFS will be the worst case for BFS "}),Object(M.jsx)("p",{children:"AVERAGE::O(n) where n is the number of nodes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures."}),Object(M.jsx)("p",{children:"It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level."}),Object(M.jsx)("p",{children:"It uses the opposite strategy of depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes."})]})]})]})})]})};var Ds=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"DEPTH FIRST SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for traversing or searching tree or graph data structures"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(V + E), where V is the number of vertices and E is the number of edges in the graph"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::Best Case for DFS will be the worst case for BFS"}),Object(M.jsx)("p",{children:"WORST::Worst Case for DFS will be the best case for BFS"}),Object(M.jsx)("p",{children:"AVERAGE::O(V) where V is the number of vertices"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures."}),Object(M.jsx)("p",{children:"The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking."}),Object(M.jsx)("p",{children:"Depth-first search is an algorithm for traversing or searching tree or graph data structures. "})]})]})]})})]})};var Ys=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"GREEDY ALGORITHM"}),Object(M.jsx)("p",{children:"A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(n) where n is the number of nodes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(bm)  where b is the branching factor (the average number of successors per state)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage."}),Object(M.jsx)("p",{children:"n many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless, a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time."}),Object(M.jsx)("p",{children:'For example, a greedy strategy for the travelling salesman problem (which is of a high computational complexity) is the following heuristic: "At each step of the journey, visit the nearest unvisited city." This heuristic does not intend to find a best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps.'})]})]})]})})]})};var Xs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"HUFFMAN ALGORITHM"}),Object(M.jsx)("p",{children:"Huffman coding is a lossless data compression algorithm."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"BEST::O(nlogn) where n is the number of node"}),Object(M.jsx)("p",{children:"WORST::O(n) where n is the number of node"}),Object(M.jsx)("p",{children:"AVERAGE::O(nlogn) where n is the number of node"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(n) where n is the number of node"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression."}),Object(M.jsx)("p",{children:"The idea is to assign variable-length codes to input characters, lengths of the assigned codes are based on the frequencies of corresponding characters."}),Object(M.jsx)("p",{children:"The most frequent character gets the smallest code and the least frequent character gets the largest code."})]})]})]})})]})};var zs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BITWISE ALGORITHM"}),Object(M.jsx)("p",{children:"Bitwise Algorithms are Algorithms used to perform operations at bit-level or to manipulate bits in different ways."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE:: O(n) where n is the total number of bits in the binary representation of the given number."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE:: O(1) auxiliary space complexity"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Bitwise Algorithms are Algorithms used to perform operations at bit-level or to manipulate bits in different ways."}),Object(M.jsx)("p",{children:"The bitwise operations are found to be much faster and are some times used to improve the efficiency of a program."})]})]})]})})]})};var Ws=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"GRAPH ALGORITHM"}),Object(M.jsx)("p",{children:"A graph is an abstract notation used to represent the connection between pairs of objects. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY OF DIJKSTRA'S ALGORITHM"})}),Object(M.jsx)("p",{children:"BEST::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"WORST::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"AVERAGE::\u0398(E+V log V) where V is the number of vertexes and E is the number of edges"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY OF DIJKSTRA'S ALGORITHM"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::\u0398(V) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The nodes are sometimes also referred to as vertices and the edges are lines or arcs that connect any two nodes in the graph."}),Object(M.jsx)("p",{children:"Interconnected objects in a graph are called vertices. Vertices are also known as nodes."}),Object(M.jsx)("p",{children:"Edges are the links that connect the vertices."})]})]})]})})]})};var Ks=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"RANDOMIZED ALGORITHM"}),Object(M.jsx)("p",{children:"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"BEST::O(E)"}),Object(M.jsx)("p",{children:"WORST::O(n Log n) where n is the number of nodes"}),Object(M.jsx)("p",{children:"AVERAGE::O(E)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"This randomness is used to reduce time complexity or space complexity in other standard algorithms"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior. "}),Object(M.jsx)("p",{children:'In the hope of achieving good performance in the "average case" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.'}),Object(M.jsx)("p",{children:"One has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite  and algorithms which have a chance of producing an incorrect result "})]})]})]})})]})};var Us=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"KARGER's ALGORITHM"}),Object(M.jsx)("p",{children:"It was invented by David Karger and first published in 1993."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE:: Karger\u2019s Algorithm for this purpose. Below Karger\u2019s algorithm can be implemented in O(E) = O(V2) time."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"Space required for step 2 need only be O(m),"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Karger\u2019s algorithm is a Monte Carlo algorithm and cut produced by it may not be minimum. "}),Object(M.jsx)("p",{children:"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph."}),Object(M.jsx)("p",{children:"The idea of the algorithm is based on the concept of contraction of an edge (u,v) in an undirected graph G=(V,E)."}),Object(M.jsx)("p",{children:"the contraction of an edge merges the nodes u and v into one, reducing the total number of nodes of the graph by one."})]})]})]})})]})};var Js=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"HAMILTONIAN CYCLE"}),Object(M.jsx)("p",{children:"A Hamiltonian cycle is a closed loop on a graph where every node (vertex) is visited exactly once."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"WORST::\u0398(E+V log V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"The Hamiltonian cycle problem is a special case of the travelling salesman problem, obtained by setting the distance between two cities to one if they are adjacent and two otherwise, and verifying that the total distance travelled is equal to n (if so, the route is a Hamiltonian circuit; if there is no Hamiltonian circuit then the shortest route will be longer)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"A Hamiltonian cycle is a closed loop on a graph where every node (vertex) is visited exactly once."}),Object(M.jsx)("p",{children:"A loop is just an edge that joins a node to itself; so a Hamiltonian cycle is a path traveling from a point back to itself, visiting every node en route."}),Object(M.jsx)("p",{children:"Hamiltonian paths and cycles are named after William Rowan Hamilton who invented the icosian game, "})]})]})]})})]})};var Qs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"FLEURYS ALGORITHM"}),Object(M.jsx)("p",{children:"Fleury's algorithm is an elegant but inefficient algorithm that dates to 1883. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(|E|2) "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(V2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Consider a graph known to have all edges in the same component and at most two vertices of odd degree."}),Object(M.jsx)("p",{children:"The algorithm starts at a vertex of odd degree, or, if the graph has none, it starts with an arbitrarily chosen vertex."}),Object(M.jsx)("p",{children:"Eulerian Path is a path in graph that visits every edge exactly once. Eulerian Circuit is an Eulerian Path which starts and ends on the same vertex."})]})]})]})})]})};var Zs=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"EULARIAN PATH"}),Object(M.jsx)("p",{children:"In graph theory, an Eulerian trail (or Eulerian path) is a trail in a finite graph that visits every edge exactly once (allowing for revisiting vertices). "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::While the graph traversal in Fleury's algorithm is linear in the number of edges, i.e. O(|E|), we also need to factor in the complexity of detecting bridges."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"Counting the number of Eulerian circuits on undirected graphs is much more difficult. "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In graph theory, an Eulerian trail (or Eulerian path) is a trail in a finite graph that visits every edge exactly once (allowing for revisiting vertices). "}),Object(M.jsx)("p",{children:"Similarly, an Eulerian circuit or Eulerian cycle is an Eulerian trail that starts and ends on the same vertex."}),Object(M.jsx)("p",{children:"They were first discussed by Leonhard Euler while solving the famous Seven Bridges of K\xf6nigsberg problem in 1736."})]})]})]})})]})};var _s=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"TARJAN's ALGORITHM"}),Object(M.jsx)("p",{children:"Tarjan's strongly connected components algorithm is an algorithm in graph theory for finding the strongly connected components (SCCs) of a directed graph."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::O(V+E) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"WORST::O(V+E) where V is the number of vertexes and E is the number of edges"}),Object(M.jsx)("p",{children:"AVERAGE::O(V+E) where V is the number of vertexes and E is the number of edges"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::|V| where V is the number of vertexes"}),Object(M.jsx)("p",{children:"WORST::|V| where V is the number of vertexes"}),Object(M.jsx)("p",{children:"AVERAGE::|V| where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Tarjan's strongly connected components algorithm is an algorithm in graph theory for finding the strongly connected components (SCCs) of a directed graph."}),Object(M.jsx)("p",{children:"It runs in linear time,matching the time bound for alternative methods including Kosaraju's algorithm and the path-based strong component algorithm. "}),Object(M.jsx)("p",{children:"The algorithm is named for its inventor, Robert Tarjan."})]})]})]})})]})};var $s=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"TRANSITIVE CLOSURE"}),Object(M.jsx)("p",{children:'In mathematics, the transitive closure of a binary relation R on a set X is the smallest relation on X that contains R and is transitive. In set theory, binary relations are represented as sets, hence "smallest" pertains to set cardinality.'})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(m)"}),Object(M.jsx)("p",{children:"WORST :- O(m)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(m)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(V \xd7 (V + E))"}),Object(M.jsx)("p",{children:"WORST :- O(V \xd7 (V + E))"}),Object(M.jsx)("p",{children:"AVERAGE :- O(V \xd7 (V + E))"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Transitive Closure it the reachability matrix to reach from vertex u to vertex v of a graph. One graph is given, we have to find a vertex v which is reachable from another vertex u, for all vertex pairs (u, v)."}),Object(M.jsx)("p",{children:"The final matrix is the Boolean type. When there is a value 1 for vertex u to vertex v, it means that there is at least one path from u to v."}),Object(M.jsx)("p",{children:"O(m)"})]})]})]})})]})};var ei=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BICONNECTED GRAPH"}),Object(M.jsx)("p",{children:"A biconnected undirected graph is a connected graph that is not broken into disconnected pieces by deleting any single vertex (and its incident edges)."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(V+E)"}),Object(M.jsx)("p",{children:"WORST :- O(n^2)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(V+E)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(n)"}),Object(M.jsx)("p",{children:"WORST :- O(n)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"An undirected graph is called Biconnected if there are two vertex-disjoint paths between any two vertices. In a Biconnected Graph, there is a simple cycle through any two vertices."}),Object(M.jsx)("p",{children:"A graph is said to be Biconnected if: "}),Object(M.jsx)("p",{children:"1) It is connected, i.e. it is possible to reach every vertex from every other vertex, by a simple path. "}),Object(M.jsx)("p",{children:"2) Even after removing any vertex the graph remains connected."})]})]})]})})]})};var ti=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"JOHNSON ALGORITHM"}),Object(M.jsx)("p",{children:"The idea of Johnson\u2019s algorithm is to re-weight all edges and make them all positive, then apply Dijkstra\u2019s algorithm for every vertex."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(V2log V + VE)"}),Object(M.jsx)("p",{children:"WORST:- O(V2log V + VE)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(V2log V + VE)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(V*VLogV)"}),Object(M.jsx)("p",{children:"WORST:- O(V*VLogV)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(V*VLogV)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The problem is to find the shortest path between every pair of vertices in a given weighted directed graph and weight may be negative. Using Johnson's Algorithm, we can find all pairs shortest path in O (V2 log ? V+VE ) time. Johnson's Algorithm uses both Dijkstra's Algorithm and Bellman-Ford Algorithm."}),Object(M.jsx)("p",{children:"The new set of edges weight w must satisfy two essential properties:"}),Object(M.jsx)("p",{children:"1) For all pair of vertices u, v \u2208 V, the shortest path from u to v using weight function w is also the shortest path from u to v using weight function w."}),Object(M.jsx)("p",{children:"2) For all edges (u, v), the new weight w (u, v) is nonnegative."})]})]})]})})]})};var si=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BELLMAN FORD ALGORITHM"}),Object(M.jsx)("p",{children:"Bellman Ford's Algorithm. Bellman Ford algorithm helps us find the shortest path from a vertex to all other vertices of a weighted graph."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- \u0398(|V||E|)"}),Object(M.jsx)("p",{children:"WORST:- \u0398(|V||E|)"}),Object(M.jsx)("p",{children:"AVERAGE :- \u0398(|V||E|)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(V) , O(V^2)"}),Object(M.jsx)("p",{children:"WORST:- O(V) , O(V^2)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(V) , O(V^2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"This algorithm works correctly when some of the edges of the directed graph G may have negative weight. When there are no cycles of negative weight, then we can find out the shortest path between source and destination."}),Object(M.jsx)("p",{children:'This algorithm detects the negative cycle in a graph and reports their existence. Based on the "Principle of Relaxation" in which more accurate values gradually       recovered an approximation to the proper distance by until eventually reaching       the optimum solution.'}),Object(M.jsx)("p",{children:"Given a weighted directed graph G = (V, E) with source s and weight function w: E \u2192 R, the Bellman-Ford algorithm returns a Boolean value indicating whether or not there is a negative weight cycle that is attainable from the source."})]})]})]})})]})};var ii=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"FORD OF FALLCURSON ALGORITHM"}),Object(M.jsx)("p",{children:"Ford-Fulkerson algorithm is a greedy approach for calculating the maximum possible flow in a network or a graph."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(E+V)"}),Object(M.jsx)("p",{children:"WORST :- O((E+V)f)"}),Object(M.jsx)("p",{children:"AVERAGE :- O(E+V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O (N * N)"}),Object(M.jsx)("p",{children:"WORST:- O (N * N)"}),Object(M.jsx)("p",{children:"AVERAGE :- O (N * N)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"A term, flow network, is used to describe a network of vertices and edges with a source (S) and a sink (T). Each vertex, except S and T, can receive and send an equal amount of stuff through it. S can only send and T can only receive stuff."}),Object(M.jsx)("p",{children:"The algorithm follows:"}),Object(M.jsx)("p",{children:"1) Initialize the flow in all the edges to 0."}),Object(M.jsx)("p",{children:"2) While there is an augmenting path between the source and the sink, add this path to the flow."}),Object(M.jsx)("p",{children:"3) Update the residual graph."})]})]})]})})]})};var ci=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"HOFFCROFT KARP ALGORITHM"}),Object(M.jsx)("p",{children:"The Hopcroft-Karp algorithm uses similar techniques as the Hungarian algorithm and Edmonds\u2019 blossom algorithm. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(\u221aV x E)"}),Object(M.jsx)("p",{children:"WORST:- O(\u221aV x E)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(\u221aV x E)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O ( V )"}),Object(M.jsx)("p",{children:"WORST:- O ( V )"}),Object(M.jsx)("p",{children:"AVERAGE:- O ( V )"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"If G is a graph and M is a maximum matching, a blocking set of augmenting paths with respect to M is a set (P1, ..., Pk) of augmenting paths such that"}),Object(M.jsx)("p",{children:"1) the paths P1, ..., Pk are vertex disjoint"}),Object(M.jsx)("p",{children:"2) they all have the same length, l l is the minimum length of an M-augmenting path"}),Object(M.jsx)("p",{children:"3) every augmenting path of length l has at least one vertex in common with P1 \u222a ... \u222a Pk"})]})]})]})})]})};var ri=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BORUVKA"}),Object(M.jsx)("p",{children:"Boruvka\u2019s Algorithm is a way to find a minimum spanning tree \u2014 a spanning tree where the sum of edge weights is minimized. It was the first algorithm developed (in 1926) to find MSTs; Otakar Boruvka used it to find the most efficient routing for an electrical grid."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(E log V)"}),Object(M.jsx)("p",{children:"WORST:- O(E log V)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(E log V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- \u0398(E + V)"}),Object(M.jsx)("p",{children:"WORST:- \u0398(E + V) "}),Object(M.jsx)("p",{children:"AVERAGE:- \u0398(E + V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Find the minimum spanning tree for the following graph using Buruvka\u2019s Algorithm."}),Object(M.jsx)("p",{children:"Step 1:- Write out a list of components. For this graph, we have: A,B,C,D,E,F,G,H,I,J,K,L. This step is optional but helps you to keep track."}),Object(M.jsx)("p",{children:"Step 2:- Highlight the cheapest outgoing edge for each node in your list. For example, node A has outgoing edges with weights 1 and 7, so we\u2019ll highlight 1. Continue sequentially (for this list, go to B, then C\u2026)."})]})]})]})})]})};var ai=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"KARATSUBA"}),Object(M.jsx)("p",{children:"The Karatsuba algorithm is a fast multiplication algorithm. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(n1.585)"}),Object(M.jsx)("p",{children:"WORST:- O(n1.585)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(n1.585) "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(n)"}),Object(M.jsx)("p",{children:"WORST:- O(n)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The Karatsuba algorithm is a fast multiplication algorithm that uses a divide and conquer approach to multiply two numbers. The naive algorithm for multiplying two numbers has a running time of \\Theta\\big(n^2\\big)\u0398(n 2 ) while this algorithm has a running time of \\Theta\\big(n^\\log_2 3\\big)\\approx \\Theta\\big(n^1.585\\big)\u0398(n log 2 \u200b 3 )\u2248\u0398(n 1.585 ). Being able to multiply numbers quickly is very important. Computer scientists often consider multiplication to be a constant time O(1)O(1) operation, and this is a reasonable simplification for smaller numbers; but for larger numbers, the actual running times need to be factored in, which is O\\big(n^2\\big)O(n 2 )."})]})]})]})})]})};var ni=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"FLOODFILL"}),Object(M.jsx)("p",{children:"In this method, a point or seed which is inside region is selected. "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :-  O(N^2)"}),Object(M.jsx)("p",{children:"WORST:-  O(N^2)"}),Object(M.jsx)("p",{children:"AVERAGE:-  O(N^2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(MN)"}),Object(M.jsx)("p",{children:"WORST:- O(MN)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(MN)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In this method, a point or seed which is inside region is selected. This point is called a seed point. Then four connected approaches or eight connected approaches is used to fill with specified color. The flood fill algorithm has many characters similar to boundary fill. But this method is more suitable for filling multiple colors boundary. When boundary is of many colors and interior is to be filled with one color we use this algorithm."}),Object(M.jsx)("p",{children:"In fill algorithm, we start from a specified interior point (x, y) and reassign all pixel values are currently set to a given interior color with the desired color. Using either a 4-connected or 8-connected approaches, we then step through pixel positions until all interior points have been repainted."}),Object(M.jsx)("p",{})]})]})]})})]})};var li=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"KRUSKAL"}),Object(M.jsx)("p",{children:"It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST :- O(E log V)"}),Object(M.jsx)("p",{children:"WORST:- O(E log V)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(E log V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(|E| + |V|)"}),Object(M.jsx)("p",{children:"WORST:- O(|E| + |V|)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(|E| + |V|)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which"}),Object(M.jsx)("p",{children:"1) form a tree that includes every vertex"}),Object(M.jsx)("p",{children:"2) has the minimum sum of weights among all the trees that can be formed from the graph"})]})]})]})})]})};var oi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"TOPOLOGICAL SORTING"}),Object(M.jsx)("p",{children:"Topological sorting for Directed Acyclic Graph (DAG) is a linear ordering of vertices such that for every directed edge u v, vertex u comes before v in the ordering."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(V+E)"}),Object(M.jsx)("p",{children:"WORST:- O(V+E)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(V+E)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(V+E)"}),Object(M.jsx)("p",{children:"WORST:- O(V+E)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(V+E)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The topological sort algorithm takes a directed graph and returns an array of the nodes where each node appears before all the nodes it points to."}),Object(M.jsx)("p",{children:"For example, a topological sorting of the following graph is \u201c5 4 2 3 1 0\u201d. There can be more than one topological sorting for a graph. For example, another topological sorting of the following graph is \u201c4 5 2 3 1 0\u201d. The first vertex in topological sorting is always a vertex with in-degree as 0 (a vertex with no incoming edges)."}),Object(M.jsx)("p",{})]})]})]})})]})};var hi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"PRIMS MINIMUM SPANNING TREE"}),Object(M.jsx)("p",{children:"The idea behind Prim's algorithm is simple, a spanning tree means all vertices must be connected."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(ElogV)"}),Object(M.jsx)("p",{children:"WORST:- O(ElogV)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(ElogV)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- \u0398(E + V)"}),Object(M.jsx)("p",{children:"WORST:- \u0398(E + V)"}),Object(M.jsx)("p",{children:"AVERAGE:- \u0398(E + V)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The idea behind Prim\u2019s algorithm is simple, a spanning tree means all vertices must be connected. So the two disjoint subsets (discussed above) of vertices must be connected to make a Spanning Tree. And they must be connected with the minimum weight edge to make it a Minimum Spanning Tree."}),Object(M.jsx)("p",{children:"1) Create a set mstSet that keeps track of vertices already included in MST. "}),Object(M.jsx)("p",{children:"2) Assign a key value to all vertices in the input graph. Initialize all key values as INFINITE. Assign key value as 0 for the first vertex so that it is picked first. "}),Object(M.jsx)("p",{children:"3) While mstSet doesn\u2019t include all vertices "}),Object(M.jsx)("p",{children:"a) Pick a vertex u which is not there in mstSet and has minimum key value. "}),Object(M.jsx)("p",{children:"b) Include u to mstSet. "}),Object(M.jsx)("p",{children:"c) Update key value of all adjacent vertices of u. To update the key values, iterate through all adjacent vertices. For every adjacent vertex v, if weight of edge u-v is less than the previous key value of v, update the key value as weight of u-v The idea of using key values is to pick the minimum weight edge from cut. The key values are used only for vertices which are not yet included in MST, the key value for these vertices indicate the minimum weight edges connecting them to the set of vertices included in MST. "})]})]})]})})]})};var di=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"K SMALLEST"}),Object(M.jsx)("p",{children:"First of all, in any interview, try to come up with brute force solution. Brute force solution to find Kth smallest element in array of integers would be to sort the array and return A[k-1] element (K-1 as array is zero base indexed)."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(N Log N)"}),Object(M.jsx)("p",{children:"WORST:- O(N Log N)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(N Log N)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(logn)"}),Object(M.jsx)("p",{children:"WORST:- O(logn)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(logn)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list. It is related to the quicksort sorting algorithm. Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm."}),Object(M.jsx)("p",{children:"Quickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side \u2013 the side with the element it is searching for. This reduces the average complexity from O(n log n) to O(n), with a worst case of O(n2)."}),Object(M.jsx)("p",{})]})]})]})})]})};var ji=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"STRASSEN's ALGORITHM"}),Object(M.jsx)("p",{children:"It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(n^2.80)"}),Object(M.jsx)("p",{children:"WORST:- O(n^2.80)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(n^2.80)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(n2)"}),Object(M.jsx)("p",{children:"WORST:- O(n2)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(n2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Strassen\u2019s matrix is a Divide and Conquer method that helps us to multiply two matrices(of size n X n)."}),Object(M.jsx)("p",{children:"You just need to remember 4 Rules :"}),Object(M.jsx)("p",{children:"(I) AHED (Learn it as \u2018Ahead\u2019)"}),Object(M.jsx)("p",{children:"(II) Diagonal"}),Object(M.jsx)("p",{children:"(III) Last CR"}),Object(M.jsx)("p",{children:"(IV) First CR"})]})]})]})})]})};var bi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"Cooley Tukey Fast Fourier Transform"}),Object(M.jsx)("p",{children:"The Cooley\u2013Tukey algorithm, named after J. W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1N2 in terms of N1 smaller DFTs of sizes N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers)."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(N log N)"}),Object(M.jsx)("p",{children:"WORST:- O(N log N)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(N log N)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(n^2)"}),Object(M.jsx)("p",{children:"WORST:- O(n^2)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(n^2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The publication by Cooley and Tukey in 1965 of an efficient algorithm for the calculation of the DFT was a major turning point in the development of digital signal processing. During the five or so years that followed, various extensions and modifications were made to the original algorithm. By the early 1970's the practical programs were basically in the form used today. The standard development shows how the DFT of a length-N sequence can be simply calculated from the two length-N/2 DFT's of the even index terms and the odd index terms. This is then applied to the two half-length DFT's to give four quarter-length DFT's, and repeated until N scalars are left which are the DFT values. Because of alternately taking the even and odd index terms, two forms of the resulting programs are called decimation-in-time and decimation-in-frequency. For a length of  2M , the dividing process is repeated  M=log2N  times and requires N multiplications each time. This gives the famous formula for the computational complexity of the FFT of  Nlog2N  which was derived in Multidimensional Index Mapping."}),Object(M.jsx)("p",{}),Object(M.jsx)("p",{})]})]})]})})]})};var Oi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"DIVIDE & CONQUROR"}),Object(M.jsx)("p",{children:"A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(nLogn)"}),Object(M.jsx)("p",{children:"WORST:- O(nLogn)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(nLogn)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(nLogn)"}),Object(M.jsx)("p",{children:"WORST:- O(nLogn)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(nLogn)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"This paradigm, divide-and-conquer, breaks a problem into subproblems that are similar to the original problem, recursively solves the subproblems, and finally combines the solutions to the subproblems to solve the original problem. Because divide-and-conquer solves subproblems recursively, each subproblem must be smaller than the original problem, and there must be a base case for subproblems. You should think of a divide-and-conquer algorithm as having three parts:"}),Object(M.jsx)("p",{children:"1) Divide the problem into a number of subproblems that are smaller instances of the same problem."}),Object(M.jsx)("p",{children:"2) Conquer the subproblems by solving them recursively. If they are small enough, solve the subproblems as base cases."}),Object(M.jsx)("p",{children:"3) Combine the solutions to the subproblems into the solution for the original problem."})]})]})]})})]})};var pi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"CHINESE REMAINDER THEOREM"}),Object(M.jsx)("p",{children:"The Chinese remainder theorem will determine a number p that, when divided by some given divisors, leaves given remainders."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(k2)"}),Object(M.jsx)("p",{children:"WORST:- O(k2)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(k2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST:- O(1)"}),Object(M.jsx)("p",{children:"WORST:- O(1)"}),Object(M.jsx)("p",{children:"AVERAGE:- O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The Chinese remainder theorem is a theorem which gives a unique solution to simultaneous linear congruences with coprime moduli. In its basic form, the Chinese remainder theorem will determine a number pp that, when divided by some given divisors, leaves given remainders."}),Object(M.jsx)("p",{children:"In number theory, the Chinese remainder theorem states that if one knows the remainders of the Euclidean division of an integer n by several integers, then one can determine uniquely the remainder of the division of n by the product of these integers, under the condition that the divisors are pairwise coprime. The earliest known statement of the theorem is by the Chinese mathematician Sun-tzu in the Sun-tzu Suan-ching in the 3rd century AD. The Chinese remainder theorem is widely used for computing with large   integers, as it allows replacing a computation for which one knows a bound on the size of the result by several similar computations on small integers. The Chinese remainder theorem (expressed in terms of congruences) is true over every principal ideal domain. It has been generalized to any commutative ring,with a formulation involving ideals."}),Object(M.jsx)("p",{}),Object(M.jsx)("p",{}),Object(M.jsx)("p",{}),Object(M.jsx)("p",{}),Object(M.jsx)("p",{})]})]})]})})]})};var mi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"AMICABLE PAIRS"}),Object(M.jsx)("p",{children:"Amicable numbers are two different numbers so related that the sum of the proper divisors of each is equal to the other number. (A proper divisor of a number is a positive factor of that number other than the number itself."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"An efficient solution is be to keep the numbers stored in a map and for every number we find the sum of its proper divisor and check"}),Object(M.jsx)("p",{children:" if that\u2019s also present in the array."}),Object(M.jsx)("p",{children:" If it is present, we can check if they form an amicable pair or not."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"COMPLEXITY"})}),Object(M.jsx)("p",{children:"TIME:- O(n^3)"}),Object(M.jsx)("p",{children:"The code is fairly straight forward and easy to understand (because I could write it lol). The time complexity analysis of the algorithm is  \u2211mi=0O(n2)."}),Object(M.jsx)("p",{children:"It's O(n^2) because each time, it checks the divisors which happens in  \u2211numi=0O(1)  and the total evaluates to O(1) + O(1)...... = O(n)."}),Object(M.jsx)("p",{children:"That roughly evaluates to O(n^3) as a complexity analysis according to the closed forms."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Let d(n) be defined as the sum of proper divisors of n (numbers less than n which divide evenly into n). If d(a) = b and d(b) = a, where a \u2260 b, then a and b are an amicable pair and each of a and b are called amicable numbers."}),Object(M.jsx)("p",{children:"For example, the proper divisors of 220 are 1, 2, 4, 5, 10, 11, 20, 22, 44, 55 and 110; therefore d(220) = 284. The proper divisors of 284 are 1, 2, 4, 71 and 142; so d(284) = 220."}),Object(M.jsx)("p",{})]})]})]})})]})};var xi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BRENT'S ALGORITHM"}),Object(M.jsx)("p",{children:"In numerical analysis, Brent's method is a hybrid root-finding algorithm combining the bisection method, the secant method and inverse quadratic interpolation."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. "}),Object(M.jsx)("p",{children:"AVERAGE::Time complexity of Brent's Algorithm is big o(m+n). Where m is the smallest index of the sequence which is the beginning of a cycle, and n is the cycle's length"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::space complexity is O(1) as you use only two pointers."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"It has the reliability of bisection but it can be as quick as some of the less-reliable methods."}),Object(M.jsx)("p",{children:"The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary."}),Object(M.jsx)("p",{children:"Brent's method is due to Richard Brent[1] and builds on an earlier algorithm by Theodorus Dekker.[2] Consequently, the method is also known as the Brent\u2013Dekker method."})]})]})]})})]})};var ui=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"REVERSE DELETE ALGORITHM"}),Object(M.jsx)("p",{children:"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Sorting the edges by weight using a comparison sort takes O(E log E) time, which can be simplified to O(E log V) using the fact that the largest E can be is V2."}),Object(M.jsx)("p",{children:"There are E iterations of the loop."}),Object(M.jsx)("p",{children:"Deleting an edge, checking the connectivity of the resulting graph, and (if it is disconnected) re-inserting the edge can be done in O(logV (log log V)3) time per operation (Thorup 2000)."}),Object(M.jsx)("p",{children:"AVERAGE::The algorithm can be shown to run in O(E log V (log log V)3) time (using big-O notation), where E is the number of edges and V is the number of vertices. "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(n) space complexity is polynomial in the size of input"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Reverse Delete algorithm is closely related to Kruskal\u2019s algorithm. In Kruskal\u2019s algorithm what we do is : Sort edges by increasing order of their weights."}),Object(M.jsx)("p",{children:"In Reverse Delete algorithm, we sort all edges in decreasing order of their weights. After sorting, we one by one pick edges in decreasing order. "}),Object(M.jsx)("p",{children:"We include current picked edge if excluding current edge causes disconnection in current graph. The main idea is delete edge if its deletion does not lead to disconnection of graph."})]})]})]})})]})};var gi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"EDMOND's ALGORITHM"}),Object(M.jsx)("p",{children:"In graph theory, Edmonds' algorithm or Chu\u2013Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight "})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"AVERAGE::The running time of this algorithm is O(EV). A faster implementation of the algorithm due to Robert Tarjan runs in time O(E \\log V) for sparse graphs and O(V^2) for dense graphs. This is as fast as Prim's algorithm for an undirected minimum spanning tree. In 1986, Gabow, Galil, Spencer, Compton, and Tarjan produced a faster implementation, with running time O(E + V \\log V)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"The algorithm takes as input a directed graph D = \\langle V, E \\rangle where V is the set of nodes and E is the set of directed edges, a distinguished vertex r \\in V called the root, and a real-valued weight w(e) for each edge e\\in E. It returns a spanning arborescence "}),Object(M.jsx)("p",{children:"It is the directed analog of the minimum spanning tree problem."}),Object(M.jsx)("p",{children:"The algorithm has a recursive description."})]})]})]})})]})};var fi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BUBBLE SORT"}),Object(M.jsx)("p",{children:"Bubble sort is a simple sorting algorithm. This sorting algorithm is comparison-based algorithm in which each pair of adjacent elements is compared and the elements are swapped if they are not in order."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Bubble sort uses two loops- inner loop and outer loop. The inner loop deterministically performs O(n) comparisons."}),Object(M.jsx)("p",{children:"Best Case Time Complexity [Big-omega]: O(n)"}),Object(M.jsx)("p",{children:"In best case, the array is already sorted but still to check, bubble sort performs O(n) comparisons. Hence, the best case time complexity of bubble sort is O(n)."}),Object(M.jsx)("p",{children:"Worst Case Time Complexity [ Big-O ]: O(n^2)"}),Object(M.jsx)("p",{children:"In worst case, the outer loop runs O(n) times. Hence, the worst case time complexity of bubble sort is O(n x n) = O(n2)."}),Object(M.jsx)("p",{children:"Average Time Complexity [Big-theta]: O(n^2)."}),Object(M.jsx)("p",{children:"In average case, bubble sort may require (n/2) passes and O(n) comparisons for each pass. Hence, the average case time complexity of bubble sort is O(n/2 x n) = \u0398(n2)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity: O(1)."}),Object(M.jsx)("p",{children:"The space complexity for Bubble Sort is O(1), because only a single additional memory space is required i.e. for temp variable."}),Object(M.jsx)("p",{children:"Bubble sort uses only a constant amount of extra space for variables like flag, i, n. Hence, the space complexity of bubble sort is O(1). It is an in-place sorting algorithm i.e. it modifies elements of the original array to sort the given array."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Some Characteristics of Bubble Sort:"})}),Object(M.jsx)("p",{children:"1. Large values are always sorted first."}),Object(M.jsx)("p",{children:"2. It only takes one iteration to detect that a collection is already sorted."}),Object(M.jsx)("p",{children:"3. The best time complexity for Bubble Sort is O(n). The average and worst time complexity is O(n\xb2). The space complexity for Bubble Sort is O(1), because only single additional memory space is required."})]})]})]})})]})};function vi(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"SELECTION SORT"}),Object(M.jsx)("p",{children:"Selection sort is a simple sorting algorithm. This sorting algorithm is an in-place comparison-based algorithm in which the list is divided into two parts, the sorted part at the left end and the unsorted part at the right end."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Selection sort algorithm consists of two nested loops. Owing to the two nested loops, it has O(n2) time complexity."}),Object(M.jsx)("p",{children:"Best case time complexity: \u03a9(n^2)"}),Object(M.jsx)("p",{children:"Worst case time complexity: \u0398(n^2)"}),Object(M.jsx)("p",{children:"Average case time complexity: O(n^2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space complexity of selection sort: O(1)"}),Object(M.jsx)("p",{children:"Selection sort is an in-place algorithm. It performs all computation in the original array and no other array is used. Hence, the space complexity works out to be O(1)."}),Object(M.jsx)("p",{children:"Selection sort is not a very efficient algorithm when data sets are large. This is indicated by the average and worst case complexities. Selection sort uses minimum number of swap operations O(n) among all the sorting algorithms."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Selection Sort Characteristics"})}),Object(M.jsx)("p",{children:"1. a small list is to be sorted"}),Object(M.jsx)("p",{children:"2. cost of swapping does not matter checking of all the elements is compulsory"}),Object(M.jsx)("p",{children:"3. cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n2) of bubble sort)"})]})]})]})})]})}function yi(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"QUICK SORT"}),Object(M.jsx)("p",{children:"Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Time taken by QuickSort in general can be written as following. T(n) = T(k) + T(n-k-1) + \u0398(n)"}),Object(M.jsx)("p",{children:"Best Case Time Complexity: \u0398(nlogn)"}),Object(M.jsx)("p",{children:" T(n) = 2T(n/2) + \u0398(n)"}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: \u0398(n^2)"}),Object(M.jsx)("p",{children:" T(n) = T(0) + T(n-1) + \u0398(n) which is equivalent to T(n) = T(n-1) + \u0398(n)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity:\u0398(nlogn)"}),Object(M.jsx)("p",{children:"T(n) = T(n/9) + T(9n/10) + \u0398(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best-case performance: O(n log n)"}),Object(M.jsx)("p",{children:"However this is only for the naive implementation of quicksort, where you recursively sort both subarrays. It is more prudent to use either tail recursion or (more commonly) iteration for the largest sub-array. Since the smallest sub-array is always less than half the size of the entire array, the recursive call stack then becomes a guaranteed  O(logn) ."}),Object(M.jsx)("p",{children:"Worst-case space complexity: O(n)"}),Object(M.jsx)("p",{children:"Usually space complexity is defined to include the size of the input, which is  O(n) . You can also look at the complexity of the auxiliary spaced used. As the other answerers have pointed out, the call stack can require an additional  O(n)  space."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics of Quick Sort"})}),Object(M.jsx)("p",{children:"1.Quick Sort is useful for sorting arrays."}),Object(M.jsx)("p",{children:"2. In efficient implementations Quick Sort is not a stable sort, meaning that the relative order of equal sort items is not preserved."}),Object(M.jsx)("p",{children:"3. Overall time complexity of Quick Sort is O(nLogn). In the worst case, it makes O(n2) comparisons, though this behavior is rare. The space complexity of Quick Sort is O(nLogn). It is an in-place sort (i.e. it doesn\u2019t require any extra storage)"})]})]})]})})]})}function wi(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"INSERTION SORT"}),Object(M.jsx)("p",{children:"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity: O(n)"}),Object(M.jsx)("p",{children:"Insertion sort performs two operations: it scans through the list, comparing each pair of elements, and it swaps elements if they are out of order. Each operation contributes to the running time of the algorithm. If the input array is already in sorted order, insertion sort compares O(n)O(n) elements and performs no swaps (in the Python code above, the inner loop is never triggered). Therefore, in the best case, insertion sort runs in O(n)O(n) time."}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"The worst case for insertion sort will occur when the input list is in decreasing order. To insert the last element, we need at most n-1n\u22121 comparisons and at most n-1n\u22121 swaps. To insert the second to last element, we need at most n-2n\u22122 comparisons and at most n-2n\u22122 swaps, and so on.[3] The number of operations needed to perform insertion sort is therefore: 2 \\times (1+2+ \\dots +n-2+n-1)2\xd7(1+2+\u22ef+n\u22122+n\u22121)"}),Object(M.jsx)("p",{children:"Use the master theorem to solve this recurrence for the running time. As expected, the algorithm's complexity is O(n^2).O(n 2 ). When analyzing algorithms, the average case often has the same complexity as the worst case. So insertion sort, on average, takes O(n^2)O(n 2 ) time."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space complexity of Insertion Sort: O(1)"}),Object(M.jsx)("p",{children:"The space complexity is actually the additional space complexity used by your algorithm, i.e. the extra space that you need, apart from the initial space occupied by the data. Bubble-sort and insertion sort use only a constant additional space, apart from the original data, so they are O(1) in space complexity."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Inserrtion Sort"})}),Object(M.jsx)("p",{children:"1. It is efficient for smaller data sets, but very inefficient for larger lists"}),Object(M.jsx)("p",{children:"2. Insertion Sort is adaptive, that means it reduces its total number of steps if given a partially sorted list, hence it increases its efficiency."}),Object(M.jsx)("p",{children:"3. Its space complexity is less."})]})]})]})})]})}function Ti(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"MERGE SORT"}),Object(M.jsx)("p",{children:"Merge sort is one of the most efficient sorting algorithms. It works on the principle of Divide and Conquer. Merge sort repeatedly breaks down a list into several sublists until each sublist consists of a single element and merging those sublists in a manner that results into a sorted list."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity[Big-omega]: O(n*log n)"}),Object(M.jsx)("p",{children:"Worst Case Time Complexity [ Big-O ]: O(n*log n) "}),Object(M.jsx)("p",{children:"Average Case Time Complexity[Big-theta]: O(n*log n) "}),Object(M.jsx)("p",{children:"Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation."}),Object(M.jsx)("p",{children:"T(n) = 2T(n/2) + O(n)"}),Object(M.jsx)("p",{children:"The solution of the above recurrence is O(nLogn). The list of size N is divided into a max of Logn parts, and the merging of all sublists into a single list takes O(N) time, the worst-case run time of this algorithm is O(nLogn)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity Of Merge Sort: O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Merge Sort"})}),Object(M.jsx)("p",{children:"1. Merge Sort is useful for sorting linked lists."}),Object(M.jsx)("p",{children:"2. Merge Sort is a stable sort which means that the same element in an array maintain their original positions with respect to each other."}),Object(M.jsx)("p",{children:"3. Overall time complexity of Merge sort is O(nLogn). It is more efficient as it is in worst case also the runtime is O(nlogn) The space complexity of Merge sort is O(n). This means that this algorithm takes a lot of space and may slower down operations for the last data sets."})]})]})]})})]})}function Si(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"HEAP SORT"}),Object(M.jsx)("p",{children:"A run of heapsort sorting an array of randomly permuted values. In the first stage of the algorithm the array elements are reordered to satisfy the heap property. Before the actual sorting takes place, the heap tree structure is shown briefly for illustration."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Time Complexity Of Heap Sort: O(nlogn)"}),Object(M.jsx)("p",{children:"Let's start with the heapify() method since we also need it for the heap's initial build. In the heapify() function, we walk through the tree from top to bottom. The height of a binary tree (the root not being counted) of size n is log2 n at most, i.e., if the number of elements doubles, the tree becomes only one level deeper: Heapsort - Zeitkomplexit\xe4t heapify()-Methode The complexity for the heapify() function is accordingly O(log n)."}),Object(M.jsx)("p",{children:"To initially build the heap, the heapify() method is called for each parent node \u2013 backward, starting with the last node and ending at the tree root. A heap of size n has n/2 (rounded down) parent nodes: Heapsort-Zeitkomplexit\xe4t: Anzahl und Reihenfolge der heapify()-Aufrufe durch buildHeap() Since the complexity of the heapify() method is O(log n) as shown above, the complexity for the buildHeap() method is, therefore, maximum O(n log n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity Of Heap Sort: O(1)"}),Object(M.jsx)("p",{children:"Only O(1) additional space is required because the heap is built inside the array to be sorted."}),Object(M.jsx)("p",{children:"Data in an array can be rearranged into a heap, in place."}),Object(M.jsx)("p",{children:"For a heap sort, you arrange the data, with the smallest element at the back. Then you swap the last item in the array (smallest item in the heap), with the first item in the array (a large number), and then shuffle that large element down the heap until it's in a new proper position and the heap is again a new min heap, with the smallest remaining element in the last element of the array."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics of Heap Sort"})}),Object(M.jsx)("p",{children:"1. Shape property: Heap is always a complete binary tree which means that all the levels of a tree are fully filled. There should not be a node which has only one child. Every node except leaves should have two children then only a heap is called as a complete binary tree."}),Object(M.jsx)("p",{children:"2. Heap property: All nodes are either greater than or equal to or less than or equal to each of its children. This means if the parent node is greater than the child node it is called as a max heap. Whereas if the parent node is lesser than the child node it is called as a min heap."}),Object(M.jsx)("p",{children:"Working Of Heao Sort:"}),Object(M.jsx)("p",{children:"Basically, there are two phases involved in the sorting of elements using heap sort algorithm they are as follows: First, start with the construction of a heap by adjusting the array elements. Once the heap is created repeatedly eliminate the root element of the heap by shifting it to the end of the array and then store the heap structure with remaining elements."})]})]})]})})]})}function Ei(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"COUNTING SORT"}),Object(M.jsx)("p",{children:"Counting sort is a stable sorting technique, which is used to sort objects according to the keys that are small numbers. It counts the number of keys whose key values are same. This sorting technique is effective when the difference between different keys are not so big, otherwise, it can increase the space complexity."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME AND SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Time Complexity of Counting Sort: O(n) "}),Object(M.jsx)("p",{children:"Space Complexity Of Counting Sort: O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"Counting sort takes O(n + k)O(n+k) time and O(n + k)O(n+k) space, where nn is the number of items we're sorting and kk is the number of possible values."}),Object(M.jsx)("p",{children:"We iterate through the input items twice\u2014once to populate counts and once to fill in the output array. Both iterations are O(n)O(n) time. Additionally, we iterate through counts once to fill in nextIndex, which is O(k)O(k) time."}),Object(M.jsx)("p",{children:"The algorithm allocates three additional arrays: one for counts, one for nextIndex, and one for the output. The first two are O(k)O(k) space and the final one is O(n)O(n) space. You can actually combine counts and nextIndex into one array. No asymptotic changes, but it does save O(k)O(k) space. In many cases cases, kk is O(n)O(n) (i.e.: the number of items to be sorted is not asymptotically different than the number of values those items can take on. Because of this, counting sort is often said to be O(n)O(n) time and space."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Counting Sort"})}),Object(M.jsx)("p",{children:"1. Counting Sort is a very efficient, stable sorting algorithm with a time and space complexity of O(n + k)."}),Object(M.jsx)("p",{children:"2. Counting Sort is mainly used for small number ranges. In the JDK, for example, for:"}),Object(M.jsx)("p",{children:"     byte arrays with more than 64 elements (for fewer elements, Insertion Sort is used) short or char arrays with more than 1,750 Elementen (for fewer elements, Insertion Sort or Dual-Pivot Quicksort is used)"})]})]})]})})]})}function Ii(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"RADIX SORT"}),Object(M.jsx)("p",{children:"Since radix sort is a non-comparative algorithm, it has advantages over comparative sorting algorithms. For the radix sort that uses counting sort as an intermediate stable sort"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME AND SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best case time complexity:\u03a9(nk)"}),Object(M.jsx)("p",{children:"Worst case time complexity:O(nk)"}),Object(M.jsx)("p",{children:"Average case time complexity:\u0398(nk)"}),Object(M.jsx)("p",{children:"Space complexity:O(n+k)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"n is the number of input data and k is the maximum element in the input data"}),Object(M.jsx)("p",{children:"Let there be d digits in input integers. Radix Sort takes O(d(n+b))* time where b is the base for representing numbers, for example, for decimal system, b is 10. What is the value of d? If k is the maximum possible value, then d would be O(logb(k)). So overall time complexity is O((n+b) * logb(k)). Which looks more than the time complexity of comparison based sorting algorithms for a large k"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Radix Sort"})}),Object(M.jsx)("p",{children:"Radix sort is one of the linear sorting algorithms for integers. It functions by sorting the input numbers on each digit, for each of the digits in the numbers. However, the process adopted by this sort method is somewhat counterintuitive, in the sense that the numbers are sorted on the least-significant digit first, followed by the second-least significant digit and so on till the most significant digit."})]})]})]})})]})}function Ai(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BUCKET SORT"}),Object(M.jsx)("p",{children:"Bucket Sort is a comparison-type algorithm which assigns elements of a list we want to sort in Buckets, or Bins. The contents of these buckets are then sorted, typically with another algorithm."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Complexity: O(n+k)"}),Object(M.jsx)("p",{children:"It occurs when the elements are uniformly distributed in the buckets with a nearly equal number of elements in each bucket. The complexity becomes even better if the elements inside the buckets are already sorted. If insertion sort is used to sort elements of a bucket then the overall complexity in the best case will be linear ie. O(n+k). O(n) is the complexity for making the buckets and O(k) is the complexity for sorting the elements of the bucket using algorithms having linear time complexity at the best case."}),Object(M.jsx)("p",{children:"Worst Case Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"When there are elements of close range in the array, they are likely to be placed in the same bucket. This may result in some buckets having more number of elements than others. It makes the complexity depend on the sorting algorithm used to sort the elements of the bucket. The complexity becomes even worse when the elements are in reverse order. If insertion sort is used to sort elements of the bucket, then the time complexity becomes O(n2)."}),Object(M.jsx)("p",{children:"Average Case Complexity: O(n)"}),Object(M.jsx)("p",{children:"It occurs when the elements are distributed randomly in the array. Even if the elements are not distributed uniformly, bucket sort runs in linear time. It holds true until the sum of the squares of the bucket sizes is linear in the total number of elements."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity Of Bucket Sort: O(n+k)"}),Object(M.jsx)("p",{children:"where n is the number of elements to be sorted and k is the number of buckets For an upper bound on the worst-case cost, it's sufficient to show that it can't be worse. Assuming that insertion sort takes \u2264cn2 steps on n elements, it is an upper bound on the cost of sorting all the buckets. For an upper bound on the worst case for bucket sort, maximize this function subject to \u2211|Bi|=n (and add the remaining cost, which is O(n) for all inputs). For a lower bound on the worst-case cost, we have to find an infinite class of actual inputs and show that their cost behaves as claimed. [0,\u2026,0] serves to show an \u03a9(n2) lower bound"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Bucket Sort"})}),Object(M.jsx)("p",{children:"1. Bucket sort assumes that the input is drawn from a uniform distribution. The computational complexity estimates involve the number of buckets."}),Object(M.jsx)("p",{children:"2. Bucket sort can be exceptionally fast because of the way elements are assigned to buckets, typically using an array where the index is the value."}),Object(M.jsx)("p",{children:"3. This means that more auxiliary memory is required for the buckets at the cost of running time than more comparison sorts."})]})]})]})})]})}var Ci=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"COMB SORT"}),Object(M.jsx)("p",{children:'Comb sort is a relatively simple sorting algorithm originally designed by W\u0142odzimierz Dobosiewicz and Artur Borowy in 1980, later rediscovered (and given the name "Combsort") by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort in the same way that Shellsort improves on insertion sort.'})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best case time complexity: \u0398(n log n)"}),Object(M.jsx)("p",{children:"Worst case time complexity: O(n^2)"}),Object(M.jsx)("p",{children:"Average case time complexity: \u03a9(n^2/2^p), where p is the number of increments"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space complexity: \u0398(1) auxillary space"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Comb Sort"})}),Object(M.jsx)("p",{children:"1. Comb Sort is an improvement over Bubble Sort."}),Object(M.jsx)("p",{children:"2. Comb Sort eliminates small values at the end of the list by using larger gap."}),Object(M.jsx)("p",{children:"3. Comb Sort has best case time complexity of \u0398(N log N) comparable to Quick Sort."}),Object(M.jsx)("p",{children:"4. Comb Sort does not require any extra space and easy to implement sorting algorithm."})]})]})]})})]})};var ki=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"TIM SORT"}),Object(M.jsx)("p",{children:"Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was implemented by Tim Peters in 2002 for use in the Python programming language."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity: O(n)"}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: O(n log n)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity: O(n log n)"}),Object(M.jsx)("p",{children:"In the worst case, Timsort takes O(n\\log n)}O(n\\log n) comparisons to sort an array of n elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm.It is advantageous over Quicksort for sorting object references or pointers because these require expensive memory indirection to access data and perform comparisons and Quicksort's cache coherence benefits are greatly reduced."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Space Complexity: O(n)"}),Object(M.jsx)("p",{children:"Worst Space Complexity: O(1)"}),Object(M.jsx)("p",{children:"Space complexity is defined as how much additional space the algorithm needs in terms of the N elements. And even though according to the docs, the sort method sorts a list in place, it does use some additional space, as stated in the description of the implementation:timsort can require a temp array containing as many as N//2 pointers, which means as many as 2*N extra bytes on 32-bit boxes. It can be expected to require a temp array this large when sorting random data; on data with significant structure, it may get away without using any extra heap memory. "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"TimSort is a sorting algorithm based on Insertion Sort and Merge Sort.A stable sorting algorithm works in O(n Log n) time"}),Object(M.jsx)("p",{children:"Used in Java\u2019s Arrays.sort() as well as Python\u2019s sorted() and sort()"}),Object(M.jsx)("p",{children:"First sort small pieces using Insertion Sort, then merges the pieces using merge of merge sort."})]})]})]})})]})};var Ri=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"CYCLE SORT"}),Object(M.jsx)("p",{children:"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME AND SAPCE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best case time complexity:O(n^2)"}),Object(M.jsx)("p",{children:"Worst case time complexity:O(n^2)"}),Object(M.jsx)("p",{children:"Best case time complexity:O(n^2)"}),Object(M.jsx)("p",{children:"Space complexity:O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"This sorting algorithm is best suited for situations where memory write or swap operations are costly."}),Object(M.jsx)("p",{children:"Here, in this comparison based sorting algorithm the time complexity will remain same for all case (i.e. Best, Average & worst cases) that is O(n^2) as in each iteration, we have to traverse the entire subarray, starting from current position, to count the no. of all the elements that are less than the current element. So, whether or not the array is already sorted or not has no consequence on the running time, nor does it provide an opportunity for optimization and the algorithm must run in quadratic time."}),Object(M.jsx)("p",{children:"Also, This sorting algorithm is inplace so it does not use any extra memory to sort the array and so that's why it's Space Complexity is constant"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Cycle Sort"})}),Object(M.jsx)("p",{children:"Cycle sort is a comparison based sorting algorithm which forces array to be factored into the number of cycles where each of them can be rotated to produce a sorted array.It is an in-place and unstable sorting algorithm."}),Object(M.jsx)("p",{children:"It is optimal in terms of number of memory writes. It minimizes the number of memory writes to sort. Each value is either written zero times, if it\u2019s already in its correct position, or written one time to its correct position."}),Object(M.jsx)("p",{children:"It is based on the idea that array to be sorted can be divided into cycles. Cycles can be visualized as a graph. We have n nodes and an edge directed from node i to node j if the element at i-th index must be present at j-th index in the sorted array."}),Object(M.jsx)("p",{children:"It is optimal in terms of number of memory writes. It makes minimum number of writes to the memory and hence efficient when array is stored in EEPROM or Flash. Unlike nearly every other sort (Quick , insertion , merge sort), items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position.This matches the minimal number of overwrites required for a completed in-place sort"})]})]})]})})]})};var Mi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BITONIC SORT"}),Object(M.jsx)("p",{children:"Bitonic sort is a comparison-based sorting algorithm that can be run in parallel. It focuses on converting a random sequence of numbers into a bitonic sequence, one that monotonically increases, then decreases. Rotations of a bitonic sequence are also bitonic."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity: O(log^2 n)"}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: O(log^2 n)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity: O(log^2 n)"}),Object(M.jsx)("p",{children:"let p=[log n] and q=[log n].It is obvious from the construction algorithm that the number of rounds of parallel comparisons is given by q(q+1)/2.It follows that the number of comparators c is bounded."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity Of Space Complexity: O(n log^2 n)"}),Object(M.jsx)("p",{children:" It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of O(n log^2 n) comparators and have a delay of O(n log^2 n), where n is the number of items to be sorted."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Bitonic Sort"})}),Object(M.jsx)("p",{children:"1. The bitonic sort is a parallel sorting algorithm that is created for best implementation and has optimum usage with hardware and parallel processor array"}),Object(M.jsx)("p",{children:"2. It is not the most effective one though as compared to the merge sort. But it is good for parallel implementation. This is due to the predefined comparison sequence which makes comparisons independent of data that are to be sorted."}),Object(M.jsx)("p",{children:"3. For bitonic sort to work effectively the number of elements should be in a specific type of quantity i.e. the order 2^n."}),Object(M.jsx)("p",{children:"4. Bitonic sequence can be rotated back to bitonic sequence"}),Object(M.jsx)("p",{children:"5. A sequence with elements in increasing and decreasing order is a bitonic sequence."})]})]})]})})]})};var Bi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"SLEEP SORT"}),Object(M.jsx)("p",{children:"Sleep Sort is time-based sorting technique. In this sorting technique , we create different threads for each individual element present in the array. The thread is then made to sleep for an amount of time that is equal to value of the element for which it was created."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Time Complexity Of Sleep Sort: O(nlogn + max(input))"}),Object(M.jsx)("p",{children:"Although there are many conflicting opinions about the time complexity of sleep sort, but we can approximate the time complexity using the below reasoning-"}),Object(M.jsx)("p",{children:"Since Sleep() function and creating multiple threads is done internally by the OS using a priority queue (used for scheduling purposes). Hence inserting all the array elements in the priority queue takes O(Nlog N) time. Also the output is obtained only when all the threads are processed, i.e- when all the elements \u2018wakes\u2019 up. Since it takes O(arr[i]) time to wake the ith array element\u2019s thread. So it will take a maximum of O(max(input)) for the largest element of the array to wake up. Thus the overall time complexity can be assumed as O(NlogN + max(input)), where, N = number of elements in the input array, and input = input array elements"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"All the things are done by the internal priority queue of the OS. Hence auxiliary space can be ignored."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Sleep Sort"})}),Object(M.jsx)("p",{children:"1. Sleep Sort is related to Operating System more than any other sorting algorithm. This sorting algorithm is a perfect demonstration of multi-threading and scheduling done by OS."}),Object(M.jsx)("p",{children:"2. This sorting technique will always sort in acsending order as the thread with the least amount of sleep time wakes up first and the number gets printed. The largest number will get printed in the end when its thread wakes up."}),Object(M.jsx)("p",{children:"3. It is also called as a Mysterious Sorting Algorithm because all the multithreading process happens in background and at the core of the OS. We do not get to know anything about what\u2019s happening in the background."})]})]})]})})]})};var Li=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"COCTAIL SORT"}),Object(M.jsx)("p",{children:"Cocktail Sort is a variation of Bubble sort. The Bubble sort algorithm always traverses elements from left and moves the largest element to its correct position in first iteration and second largest in second iteration and so on. Cocktail Sort traverses through a given array in both directions alternatively."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity: O(n)"}),Object(M.jsx)("p",{children:"The best configuration occurs when all elements are already sorted or nearly sorted.Consequently, we have to go through the first sequence only once: O(n)."}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"Just like the Bubble sort, the worst case O(n2) occurs when the sequence is in reverse order. First, the highest value is bubbled down until the end, and the lowest value is bubbled up to the beginning; then the loop starts again starting at (begin + 1) to (end - 1) and so forth."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Cocktail Sort does not use any buffer nor does make any recursion. Thus, it requires O(1) space in all case."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Coctail Sort"})}),Object(M.jsx)("p",{children:"1. Cocktail sort, just like the Bubble sort, works by iterating through the list, comparing adjacent elements and swapping them if they are in the wrong order. The only real difference is that it does it in both directions instead of only going from left to right."}),Object(M.jsx)("p",{children:"2. Because of this, cocktail sort manages to get around the \u201cturtles problem\u201d (small elements near the end of the list slowing down the algorithm) of bubble sort. However, it still retains the same worst-case computational complexity"}),Object(M.jsx)("p",{children:"3. another way of thinking of this sorting algorithm is:Each time I shake the cocktail: the more massive value goes down when I shake down and the lighter value goes up when I shake up."})]})]})]})})]})};var Ni=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"STRAND SORT"}),Object(M.jsx)("p",{children:"A sort algorithm that works well if many items are in order. First, begin a sublist by moving the first item from the original list to the sublist. For each subsequent item in the original list, if it is greater than the last item of the sublist, remove it from the original list and append it to the sublist. Merge the sublist into a final, sorted list. Repeatedly extract and merge sublists until all items are sorted. Handle two or fewer items as special cases."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Time Complexity: O(n) "}),Object(M.jsx)("p",{children:"Worst Case Time Complexity: O(n^2)"}),Object(M.jsx)("p",{children:"Average Case Time Complexity: O(n sqrt n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Strand sort is not in-place as its space complexity is O(n). The algorithm first moves the first element of a list into a sub-list. It then compares the last element in the sub-list to each subsequent element in the original list."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Strand Sort"})}),Object(M.jsx)("p",{children:"1. Strand sort is a sorting algorithm. It works by repeatedly pulling sorted sublists out of the list to be sorted and merging them with a result array. Each iteration through the unsorted list pulls out a series of elements which were already sorted, and merges those series together."}),Object(M.jsx)("p",{children:'2. The name of the algorithm comes from the "strands" of sorted data within the unsorted list which are removed one at a time. It is a comparison sort due to its use of comparisons when removing strands and when merging them into the sorted array.'}),Object(M.jsx)("p",{children:"3. Strand sort is most useful for data which is stored in a linked list, due to the frequent insertions and removals of data. Using another data structure, such as an array, would greatly increase the running time and complexity of the algorithm due to lengthy insertions and deletions. Strand sort is also useful for data which already has large amounts of sorted data, because such data can be removed in a single strand"})]})]})]})})]})};var Pi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"SHELL SORT"}),Object(M.jsx)("p",{children:"Shell sort is a highly efficient sorting algorithm and is based on insertion sort algorithm. This algorithm avoids large shifts as in case of insertion sort, if the smaller value is to the far right and has to be moved to the far left."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best Case Complexity: O(n*log n)"}),Object(M.jsx)("p",{children:"When the array is already sorted, the total number of comparisons for each interval (or increment) is equal to the size of the array."}),Object(M.jsx)("p",{children:"Worst Case Complexity: less than or equal to O(n2)"}),Object(M.jsx)("p",{children:"Worst case complexity for shell sort is always less than or equal to O(n2).According to Poonen Theorem, worst case complexity for shell sort is \u0398(Nlog N)2/(log log N)2) or \u0398(Nlog N)2/log log N) or \u0398(N(log N)2) or something in between."}),Object(M.jsx)("p",{children:"Average Case Complexity: O(n*log n)"}),Object(M.jsx)("p",{children:"It is around O(n1.25)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"The space complexity for shell sort is O(1)."}),Object(M.jsx)("p",{children:"This algorithm is quite efficient for medium-sized data sets as its average and worst-case complexity of this algorithm depends on the gap sequence the best known is \u039f(n), where n is the number of items. And the worst case space complexity is O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Shell Sort"})}),Object(M.jsx)("p",{children:"1. Shellsort performs more operations and has higher cache miss ratio than quicksort."}),Object(M.jsx)("p",{children:"2. However, since it can be implemented using little code and does not use the call stack, some implementations of the qsort function in the C standard library targeted at embedded systems use it instead of quicksort. Shellsort is, for example, used in the uClibc library. For similar reasons, an implementation of Shellsort is present in the Linux kernel."}),Object(M.jsx)("p",{children:"3. Shellsort can also serve as a sub-algorithm of introspective sort, to sort short subarrays and to prevent a slowdown when the recursion depth exceeds a given limit. This principle is employed, for instance, in the bzip2 compressor."})]})]})]})})]})};var Vi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"TREE SORT"}),Object(M.jsx)("p",{children:"A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Worst Case Time Complexity : O(n^2)"}),Object(M.jsx)("p",{children:"The worst case time complexity of Tree Sort can be improved by using a self-balancing binary search tree like Red Black Tree, AVL Tree. Using self-balancing binary tree Tree Sort will take O(n log n) time to sort the array in worst case."}),Object(M.jsx)("p",{children:"Average Case Time Complexity : O(n log n)"}),Object(M.jsx)("p",{children:"Adding one item to a Binary Search tree on average takes O(log n) time. Therefore, adding n items will take O(n log n) time"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"The space complexity of tree sort is O(n) & O(n) in both the average and the worst cases."}),Object(M.jsx)("p",{children:"WORST."}),Object(M.jsx)("p",{children:"AVERAGE."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Tree Sort"})}),Object(M.jsx)("p",{children:"1.The properties of binary search tree is completely make use of tree sort algorithm. The tree sort algorithm first builds a binary search tree using the elements in an array which is to be sorted and then does an in-order traversal so that the numbers are retrived in a sorted order "}),Object(M.jsx)("p",{children:"2. The main advantage of tree sort algorithm is that we can make changes very easily as in a linked list.Sorting in Tree sort algorithm is as fast as quick sort algorithm."}),Object(M.jsx)("p",{children:"3. The worst case occur when the elements in an array is already sorted.In worst case, the running time of tree sort algorithm is 0 (n2)."})]})]})]})})]})};var Gi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"PERMUTATION SORT"}),Object(M.jsx)("p",{children:"In computer science, bogosort (also known as permutation sort, stupid sort, or slowsort) is a highly inefficient sorting algorithm based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME AND SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best case time complexity: O(n)"}),Object(M.jsx)("p",{children:"Worst case time complexity: O(Infinity)"}),Object(M.jsx)("p",{children:"Average case time complexity: O(n!)"}),Object(M.jsx)("p",{children:"Space Complexity: O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"The worst case is the case when the list is never sorted as the same subset of elements are swapped which makes the list stuck in a loop. This never leads to the list being sorted."}),Object(M.jsx)("p",{children:"The best case is the case when the list is already sorted or the first swap makes the list sorted."}),Object(M.jsx)("p",{children:"This algorithm does not use any extra memory so it is memory efficient"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Permutation Sort"})}),Object(M.jsx)("p",{children:"1. Bogosort is an algorithm used as a demonstration of the least effective approach to sort a list of values. The Bogosort is only a theoretical concept, which has no use in practical applications."}),Object(M.jsx)("p",{children:"2. The bogosort procedure is trivial \u2013 at first it checks, if the list is already sorted. If so, algorithm terminates. Otherwise it randomly shuffles the list and repeats the procedure."}),Object(M.jsx)("p",{children:"3. Bogosort is only a theoretical concept, which has no use in practical applications."})]})]})]})})]})};var Hi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"PANCAKE SORT"}),Object(M.jsx)("p",{children:'Pancake sort is a sorting algorithm in which the only allowed operation is to "flip" one end of the list. It is inplace but not stable . Pancake sort is called so because it resembles sorting pancakes on a plate with a spatula, where you can only use the spatula to flip some of the top pancakes in the plate.'})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"O(n^2) where n is the size of the given array"}),Object(M.jsx)("p",{children:"In the algorithm of pancake sort , there we run a loop with n iterations"}),Object(M.jsx)("p",{children:"Within each iteration, we are dealing with the corresponding prefix of the list. Here we denote the length of the prefix as k, e.g. in the first iteration, the length of the prefix is n. While in the second iteration, the length of the prefix is n-1."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"O(n) where n is the size of the given array."}),Object(M.jsx)("p",{children:"Within the algorithm, we use a list to maintain the final results, which are proportional to the number of pancake flips."}),Object(M.jsx)("p",{children:"For each round of iteration, at most, we would add two pancake flips. Therefore, the maximal number of pancake flips needed would be 2*n."}),Object(M.jsx)("p",{children:"As a result, the space complexity of the algorithm is O(n). If one does not take into account the space required to hold the result of the function, then one could consider the above algorithm as a constant space solution."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"Characteristics Of Pancake sort"})}),Object(M.jsx)("p",{children:"1. It is a variation of the sorting problem in which the only allowed operation is to reverse the elements of some prefix of the sequence"}),Object(M.jsx)("p",{children:"2. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible."}),Object(M.jsx)("p",{children:"3. It also states that, it has applications in parallel processor networks, in which it can provide an effective routing algorithm between processors."})]})]})]})})]})};var Fi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"GNOME SORT"}),Object(M.jsx)("p",{children:"The gnome sort is a sorting algorithm which is similar to insertion sort in that it works with one item at a time but gets the item to the proper place by a series of swaps, similar to a bubble sort. It is conceptually simple, requiring no nested loops."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"Best case time complexity: \u0398(n)"}),Object(M.jsx)("p",{children:"Worst case time complexity: \u0398(n^2)"}),Object(M.jsx)("p",{children:"Average case time complexity: \u0398(n^2)"}),Object(M.jsx)("p",{children:"As there are no nested loop (only one while) it may seem that this is a linear O(N) time algorithm. But the time complexity is O(N^2). This is because the variable \u2013 \u2018index\u2019 in our program doesn\u2019t always gets incremented, it gets decremented too."}),Object(M.jsx)("p",{children:"However this sorting algorithm is adaptive and performs better if the array is already/partially sorted."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space complexity: \u0398(1) auxiliary"}),Object(M.jsx)("p",{children:"This is an in-place algorithm. So O(1) auxiliary space is needed."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"1. Gnome sort is used everywhere where a stable sort is not needed."}),Object(M.jsx)("p",{children:"2. Gnome Sort is an in-place sort that is does not require any extra storage."}),Object(M.jsx)("p",{children:"3. The gnome sort is a sorting algorithm which is similar to insertion sort in that it works with one item at a time but gets the item to the proper place by a series of swaps, similar to a bubble sort."}),Object(M.jsx)("p",{children:"4. It is conceptually simple, requiring no nested loops. The average running time is O(n^2) but tends towards O(n) if the list is initially almost sorted."})]})]})]})})]})};var qi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"EVEN-ODD SORT/BRICK SORT"}),Object(M.jsx)("p",{children:"In computing, an odd\u2013even sort or odd\u2013even transposition sort (also known as brick sort or parity sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(N2) where, N = Number of elements in the input array. "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::Auxiliary Space O(1) Just like bubble sort this is also an in-place algorithm"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. This is basically a variation of bubble-sort."}),Object(M.jsx)("p",{children:"This algorithm is divided into two phases- Odd and Even Phase."}),Object(M.jsx)("p",{children:"The algorithm runs until the array elements are sorted and in each iteration two phases occurs- Odd and Even Phases."})]})]})]})})]})};var Di=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BINARY INSERTION SORT ALGORITHM"}),Object(M.jsx)("p",{children:"We can use binary search to reduce the number of comparisons in normal insertion sort."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"BEST:: \u0398(N)"}),Object(M.jsx)("p",{children:"WORST:: \u0398(N log N) "}),Object(M.jsx)("p",{children:"AVERAGE:: \u0398(N log N)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE:: O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Binary Insertion Sort uses binary search to find the proper location to insert the selected item at each iteration. "}),Object(M.jsx)("p",{children:"In normal insertion sort, it takes O(n) comparisons (at nth iteration) in the worst case. We can reduce it to O(log n) by using binary search."})]})]})]})})]})};var Yi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"PEGIONHOLE SORT ALGORITHM"}),Object(M.jsx)("p",{children:"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements and the number of possible key values are approximately the same."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::Time Complexity, O(n+2^k) "}),Object(M.jsx)("p",{children:"WORST::n+2^k"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::Space Complexity, O(2^k)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same"}),Object(M.jsx)("p",{children:'It requires O(n + N) time. It is similar to counting sort, but differs in that it "moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item\'s final destination and move the item there."'}),Object(M.jsx)("p",{children:"It requires O(n + Range) time where n is number of elements in input array and \u2018Range\u2019 is number of possible values in array."})]})]})]})})]})};var Xi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"LINEAR SEARCH"}),Object(M.jsx)("p",{children:"THIS IS LINEAR SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : O(n)"}),Object(M.jsx)("p",{children:"AVERAGE: O(n/2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : O(1) iterative"}),Object(M.jsx)("p",{children:"AVERAGE: O(n/2)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In computer science, a linear search or sequential search is a method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched."}),Object(M.jsx)("p",{children:"A linear search runs in at worst linear time and makes at most n comparisons, where n is the length of the list. If each element is equally likely to be searched, then linear search has an average case of n+1 / 2 comparisons, but the average case can be affected if the search probabilities for each element vary. "}),Object(M.jsx)("p",{children:"Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists."})]})]})]})})]})};var zi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"JUMP SEARCH"}),Object(M.jsx)("p",{children:"THIS IS JUMP SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : O(\u221an)"}),Object(M.jsx)("p",{children:"AVERAGE: O(\u221an) "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"BEST : O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Like Binary Search, Jump Search is a searching algorithm for sorted arrays."}),Object(M.jsx)("p",{children:"The basic idea is to check fewer elements (than linear search) by jumping ahead by fixed steps or skipping some elements in place of searching all elements."}),Object(M.jsx)("p",{children:"Works only sorted arrays. The optimal size of a block to be jumped is (\u221a n). This makes the time complexity of Jump Search O(\u221a n). The time complexity of Jump Search is between Linear Search ( ( O(n) ) and Binary Search ( O (Log n) )."})]})]})]})})]})};var Wi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"INTERPOLATION SEARCH"}),Object(M.jsx)("p",{children:"THIS IS INTERPOLATION SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(10"}),Object(M.jsx)("p",{children:"WORST : O(n)"}),Object(M.jsx)("p",{children:"AVERAGE: O(log(log(n)))"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"BEST : O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Given a sorted array of n uniformly distributed values arr[], write a function to search for a particular element x in the array. "}),Object(M.jsx)("p",{children:"Linear Search finds the element in O(n) time, Jump Search takes O(\u221a n) time and Binary Search take O(Log n) time. "}),Object(M.jsx)("p",{children:"The Interpolation Search is an improvement over Binary Search for instances, where the values in a sorted array are uniformly distributed. Binary Search always goes to the middle element to check. On the other hand, interpolation search may go to different locations according to the value of the key being searched. For example, if the value of the key is closer to the last element, interpolation search is likely to start search toward the end side."})]})]})]})})]})};var Ki=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BINARY SEARCH"}),Object(M.jsx)("p",{children:"THIS IS BINARY SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : Olog(n)"}),Object(M.jsx)("p",{children:"AVERAGE: Olog(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : O(n)"}),Object(M.jsx)("p",{children:"AVERAGE: O(n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Search a sorted array by repeatedly dividing the search interval in half. Begin with an interval covering the whole array. "}),Object(M.jsx)("p",{children:"If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half."}),Object(M.jsx)("p",{children:"Otherwise narrow it to the upper half. Repeatedly check until the value is found or the interval is empty.The idea of binary search is to use the information that the array is sorted and reduce the time complexity to O(Log n)."})]})]})]})})]})};var Ui=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"EXPONENTIAL SEARCH"}),Object(M.jsx)("p",{children:"THIS IS EXPONENTIAL SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST : O(1)"}),Object(M.jsx)("p",{children:"WORST : O(Log2 i)"}),Object(M.jsx)("p",{children:"AVERAGE: O(log2 i)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"WORST : O(1)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"In computer science, an exponential search (also called doubling search or galloping search or Struzik search)[1] is an algorithm, created by Jon Bentley and Andrew Chi-Chih Yao in 1976, for searching sorted, unbounded/infinite lists."}),Object(M.jsx)("p",{children:"There are numerous ways to implement this with the most common being to determine a range that the search key resides in and performing a binary search within that range."}),Object(M.jsx)("p",{children:"This takes O(log i) where i is the position of the search key in the list, if the search key is in the list, or the position where the search key should be, if the search key is not in the list."})]})]})]})})]})};var Ji=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"SUBLIST SEARCH"}),Object(M.jsx)("p",{children:"THIS IS SUBLIST SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm.",Object(M.jsx)("p",{children:"BEST : O(n)"}),Object(M.jsx)("p",{children:"WORST : O(n*m)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Take first node of second list. 2- Start matching the first list from this first node. 3- If whole lists match return true."}),Object(M.jsx)("p",{children:"Else break and take first list to the first node again. 5- And take second list to its second node. 6- Repeat these steps until any of linked lists becomes empty."}),Object(M.jsx)("p",{children:"If first list becomes empty then list found else not."})]})]})]})})]})};var Qi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"FIBONACCI SEARCH"}),Object(M.jsx)("p",{children:"THIS IS FIBONACCI SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm.",Object(M.jsx)("p",{children:"WORST : O(log n)"}),Object(M.jsx)("p",{children:"AVERAGE: O(log n)"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"O(n) for Recursive Fibonacci "})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Fibonacci Search is a comparison-based technique that uses Fibonacci numbers to search an element in a sorted array."}),Object(M.jsx)("p",{children:"Compared to binary search where the sorted array is divided into two equal-sized parts, one of which is examined further, Fibonacci search divides the array into two parts that have sizes that are consecutive Fibonacci numbers. On average, this leads to about 4% more comparisons to be executed"}),Object(M.jsx)("p",{children:"but it has the advantage that one only needs addition and subtraction to calculate the indices of the accessed array elements, while classical binary search needs bit-shift, division or multiplication,"})]})]})]})})]})};var Zi=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"K NEIGHBOR SEARCH"}),Object(M.jsx)("p",{children:"THIS IS K NEIGHBOR SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"O(n\xd7m), where n is the number of training examples and m is the number of dimensions in the training set."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"O(n*d) where n represents number of datapoints and d represents the number of features that determine each datapoint"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"K-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection."}),Object(M.jsx)("p",{children:"It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data)."})]})]})]})})]})};var _i=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"UBIQUITOUS BINARY SEARCH"}),Object(M.jsx)("p",{children:"THIS IS UBIQUITOUS BINARY SEARCHING ALGORITHM"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"LOGIC"})}),Object(M.jsx)("p",{children:"Problem Statement: Given a sorted array of N distinct elements. Find a key in the array using least number of comparisons. (Do you think binary search is optimal to search a key in sorted array?)Without much theory, here is typical binary search algorithm."}),Object(M.jsx)("p",{children:"Theoretically we need log N + 1 comparisons in worst case. If we observe, we are using two comparisons per iteration except during final successful match, if any. In practice, comparison would be costly operation, it won\u2019t be just primitive type comparison. It is more economical to minimize comparisons as that of theoretical limit."}),Object(M.jsx)("p",{children:"In the while loop we are depending only on one comparison. The search space converges to place l and r point two different consecutive elements. We need one more comparison to trace search status."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"CASE AND CONDITIONS"})}),Object(M.jsx)("p",{children:"If all elements in the array are smaller than key, left pointer moves till last element."}),Object(M.jsx)("p",{children:"If all elements in the array are greater than key, it is an error condition."}),Object(M.jsx)("p",{children:"If all elements in the array equal and key, it is worst case input to our implementation"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"EXAMPLES"})}),Object(M.jsx)("p",{children:" Given a sorted array of distinct elements, and the array is rotated at an unknown position. Find minimum element in the array."}),Object(M.jsx)("p",{children:"We can see  pictorial representation of sample input array in the below figure."}),Object(M.jsx)("p",{children:"We converge the search space till l and r points single element. If the middle location falls in the first pulse, we converge our search space to A[m+1 \u2026 r]. If the middle location falls in the second pulse, we converge our search space to A[1 \u2026 m]. At every iteration we check for search space size, if it is 1, we are done."})]})]})]})})]})};var $i=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"A*SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency."})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(bd) where b is the branching factor (the average number of successors per state)."})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"Space complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two dimensional array of size n*n, this will require O(n2) space."}),Object(M.jsx)("p",{children:"AVERAGE::O(|V|) = O(b^d) where V is the number of vertexes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:'A* (pronounced "A-star") is a graph traversal and path search algorithm. A* is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency.'}),Object(M.jsx)("p",{children:" One major practical drawback is its O(b^d) space complexity, as it stores all generated nodes in memory."}),Object(M.jsx)("p",{children:"Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance."})]})]})]})})]})};var ec=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"BREADTH FIRST SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for traversing or searching tree or graph data structures"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::O(N) where N is the number of nodes"}),Object(M.jsx)("p",{children:"WORST::O(N) where N is the number of nodes"}),Object(M.jsx)("p",{children:"AVERAGE::O(N) where N is the number of nodes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. "}),Object(M.jsx)("p",{children:"BEST::Worst Case for DFS will be the best case for BFS"}),Object(M.jsx)("p",{children:"WORST::Best Case for DFS will be the worst case for BFS "}),Object(M.jsx)("p",{children:"AVERAGE::O(n) where n is the number of nodes"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures."}),Object(M.jsx)("p",{children:"It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level."}),Object(M.jsx)("p",{children:"It uses the opposite strategy of depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes."})]})]})]})})]})};var tc=function(){return Object(M.jsxs)("section",{children:[Object(M.jsx)("div",{class:"routeloader",children:Object(M.jsxs)("div",{class:"coder-logo-animation",children:[Object(M.jsxs)("div",{class:"left",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]}),Object(M.jsxs)("div",{class:"right",children:[Object(M.jsx)("div",{class:"part-1"}),Object(M.jsx)("div",{class:"part-2"}),Object(M.jsx)("div",{class:"part-3"})]})]})}),Object(M.jsx)(Vs,{waitBeforeShow:2e3,children:Object(M.jsxs)("div",{class:"container",children:[Object(M.jsxs)("div",{class:"jumbotron",children:[Object(M.jsx)("h1",{children:"DEPTH FIRST SEARCH ALGORITHM"}),Object(M.jsx)("p",{children:"An algorithm for traversing or searching tree or graph data structures"})]}),Object(M.jsxs)("div",{class:"row",children:[Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"TIME COMPLEXITY"})}),Object(M.jsx)("p",{children:"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor."}),Object(M.jsx)("p",{children:"AVERAGE::O(V + E), where V is the number of vertices and E is the number of edges in the graph"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"SPACE COMPLEXITY"})}),Object(M.jsx)("p",{children:"BEST::Best Case for DFS will be the worst case for BFS"}),Object(M.jsx)("p",{children:"WORST::Worst Case for DFS will be the best case for BFS"}),Object(M.jsx)("p",{children:"AVERAGE::O(V) where V is the number of vertices"})]}),Object(M.jsxs)("div",{class:"col-sm-4",children:[Object(M.jsx)("h3",{children:Object(M.jsx)("u",{children:"DEFINATION"})}),Object(M.jsx)("p",{children:"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures."}),Object(M.jsx)("p",{children:"The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking."}),Object(M.jsx)("p",{children:"Depth-first search is an algorithm for traversing or searching tree or graph data structures. "})]})]})]})})]})},sc=function(){return Object(M.jsx)("div",{id:"main",children:Object(M.jsx)("div",{class:"fof",children:Object(M.jsx)("h1",{children:"Error 404"})})})},ic=document.getElementById("root");setTimeout((function(){return a.a.render(Object(M.jsx)(ot.a,{children:Object(M.jsxs)(Ms.c,{children:[Object(M.jsx)(Ms.a,{exact:!0,path:"/",component:As}),Object(M.jsx)(Ms.a,{path:"/firstOtherDIJKSTRA",component:Gs}),Object(M.jsx)(Ms.a,{path:"/secondOtherFLOYDWARSHALL",component:Hs}),Object(M.jsx)(Ms.a,{path:"/thirdOtherASTARSEARCH",component:Fs}),Object(M.jsx)(Ms.a,{path:"/fourthOtherbfs",component:qs}),Object(M.jsx)(Ms.a,{path:"/fifthOtherdfs",component:Ds}),Object(M.jsx)(Ms.a,{path:"/sixthOthergreedy",component:Ys}),Object(M.jsx)(Ms.a,{path:"/seventhOtherhofman",component:Xs}),Object(M.jsx)(Ms.a,{path:"/eighthOtherbit",component:zs}),Object(M.jsx)(Ms.a,{path:"/ninthOthergraph",component:Ws}),Object(M.jsx)(Ms.a,{path:"/tenthOtherrandomized",component:Ks}),Object(M.jsx)(Ms.a,{path:"/eleventhOtherkargers",component:Us}),Object(M.jsx)(Ms.a,{path:"/twelvthOtherhamiltoniancycle",component:Js}),Object(M.jsx)(Ms.a,{path:"/thirteenthOtherfleurys",component:Qs}),Object(M.jsx)(Ms.a,{path:"/fourteenthOthereularianpath",component:Zs}),Object(M.jsx)(Ms.a,{path:"/fifteenthOthertarjan",component:_s}),Object(M.jsx)(Ms.a,{path:"/sixteenOthertransitiveclosure",component:$s}),Object(M.jsx)(Ms.a,{path:"/seventeenOtherbiconnectedgraph",component:ei}),Object(M.jsx)(Ms.a,{path:"/eightteenOtherjohnsons",component:ti}),Object(M.jsx)(Ms.a,{path:"/nineteenOtherbellmanford",component:si}),Object(M.jsx)(Ms.a,{path:"/twentyOtherfordfallcurson",component:ii}),Object(M.jsx)(Ms.a,{path:"/twentyoneOtherhoffcroftkarp",component:ci}),Object(M.jsx)(Ms.a,{path:"/twentytwoOtherboruvka",component:ri}),Object(M.jsx)(Ms.a,{path:"/twentythreeOtherkaratsuba",component:ai}),Object(M.jsx)(Ms.a,{path:"/twentyfourOtherfloodfill",component:ni}),Object(M.jsx)(Ms.a,{path:"/twentyfiveOtherkruskal",component:li}),Object(M.jsx)(Ms.a,{path:"/twentysixOthertopologicalsorting",component:oi}),Object(M.jsx)(Ms.a,{path:"/twentysevenOtherprimsminimumspanningtree",component:hi}),Object(M.jsx)(Ms.a,{path:"/twentyeightOtherksmallest",component:di}),Object(M.jsx)(Ms.a,{path:"/twentynineOtherstrassen",component:ji}),Object(M.jsx)(Ms.a,{path:"/thirtyOthercooleytukeyfastfouriertransform",component:bi}),Object(M.jsx)(Ms.a,{path:"/thirtyoneOtherdivideconquror",component:Oi}),Object(M.jsx)(Ms.a,{path:"/thirtytwoOtherchineseremaindertheorem",component:pi}),Object(M.jsx)(Ms.a,{path:"/thirtythreeOtheramicablepairs",component:mi}),Object(M.jsx)(Ms.a,{path:"/thirtifourthOtherBRENT",component:xi}),Object(M.jsx)(Ms.a,{path:"/thirtififthOtherREVERSEDELETE",component:ui}),Object(M.jsx)(Ms.a,{path:"/thirtisixthOtherEDMOND",component:gi}),Object(M.jsx)(Ms.a,{path:"/firstSortingBubbleSort",component:fi}),Object(M.jsx)(Ms.a,{path:"/secondSortingSelectionsort",component:vi}),Object(M.jsx)(Ms.a,{path:"/thirdSortingQuickSort",component:yi}),Object(M.jsx)(Ms.a,{path:"/fourthSortingInsertionSort",component:wi}),Object(M.jsx)(Ms.a,{path:"/fifthSortingMergerSort",component:Ti}),Object(M.jsx)(Ms.a,{path:"/sixthSortingHeapSort",component:Si}),Object(M.jsx)(Ms.a,{path:"/seventhSortingCountingSort",component:Ei}),Object(M.jsx)(Ms.a,{path:"/eighthSortingRadixSort",component:Ii}),Object(M.jsx)(Ms.a,{path:"/ninethSortingBucketSort",component:Ai}),Object(M.jsx)(Ms.a,{path:"/tenthSortingCombSort",component:Ci}),Object(M.jsx)(Ms.a,{path:"/eleventhSortingTimSort",component:ki}),Object(M.jsx)(Ms.a,{path:"/twelvethSortingCycleSort",component:Ri}),Object(M.jsx)(Ms.a,{path:"/thirteenthSortingBitonicSort",component:Mi}),Object(M.jsx)(Ms.a,{path:"/fourteenthSortingSleepSort",component:Bi}),Object(M.jsx)(Ms.a,{path:"/fifteenthSortingCoctailSort",component:Li}),Object(M.jsx)(Ms.a,{path:"/sixteenthSortingStrandSort",component:Ni}),Object(M.jsx)(Ms.a,{path:"/seventeenthSortingShellSort",component:Pi}),Object(M.jsx)(Ms.a,{path:"/eighteenthSortingTreeSort",component:Vi}),Object(M.jsx)(Ms.a,{path:"/nineteenthSortingPermutationSort",component:Gi}),Object(M.jsx)(Ms.a,{path:"/twentythSortingPancakeSort",component:Hi}),Object(M.jsx)(Ms.a,{path:"/twentyonethSortingGnomeSort",component:Fi}),Object(M.jsx)(Ms.a,{path:"/twentysecondSortingbricksort",component:qi}),Object(M.jsx)(Ms.a,{path:"/twentythirdSortingbinaryinsertionsort",component:Di}),Object(M.jsx)(Ms.a,{path:"/twentyfourthSortingpegionhole",component:Yi}),Object(M.jsx)(Ms.a,{path:"/firstSearchingAlgo",component:Xi}),Object(M.jsx)(Ms.a,{path:"/secondSearchingAlgo",component:zi}),Object(M.jsx)(Ms.a,{path:"/thirdSearchingAlgo",component:Wi}),Object(M.jsx)(Ms.a,{path:"/fourthSearchingBinarySearch",component:Ki}),Object(M.jsx)(Ms.a,{path:"/fifthSearchingAlgo",component:Ui}),Object(M.jsx)(Ms.a,{path:"/sixthSearchingAlgo",component:Ji}),Object(M.jsx)(Ms.a,{path:"/seventhSearchingAlgo",component:Qi}),Object(M.jsx)(Ms.a,{path:"/eighthSearchingAlgo",component:Zi}),Object(M.jsx)(Ms.a,{path:"/ninthSearchingAlgo",component:_i}),Object(M.jsx)(Ms.a,{path:"/tenthSearchingastarsearch",component:$i}),Object(M.jsx)(Ms.a,{path:"/eleventhSearchingbfs",component:ec}),Object(M.jsx)(Ms.a,{path:"/twelvthSearchingdfs",component:tc}),Object(M.jsx)(Ms.a,{component:sc})]})}),ic)}),3e3)}},[[33,1,2]]]);
//# sourceMappingURL=main.499402ff.chunk.js.map